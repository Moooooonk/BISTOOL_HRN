{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRN (Hierarchical Representation Network) - 3D Face Reconstruction\n",
    "## arXiv:2302.14434v2 - Lei et al., DAMO Academy, Alibaba Group\n",
    "\n",
    "**ëª©ì°¨**\n",
    "1. í™˜ê²½ ì„¤ì • & Google Drive ë§ˆìš´íŠ¸\n",
    "2. ë°ì´í„° íƒìƒ‰ (DICOM CT + FaceScape)\n",
    "3. CT ë°ì´í„° ì‹œê°í™” ë° 2D ì–¼êµ´ ì´ë¯¸ì§€ ì¶”ì¶œ\n",
    "4. HRN ëª¨ë¸ ì„¤ì¹˜ ë° 3D ì–¼êµ´ ë³µì›\n",
    "5. FaceScape ë°ì´í„° ì‹¤í—˜\n",
    "6. ìµœì‹  ê¸°ë²• ë¹„êµ (CodeFormer, GFPGAN)\n",
    "7. ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "> **ì£¼ì˜**: ëª¨ë“  ë°ì´í„°ëŠ” Google Driveì—ì„œ ì½ê³ , ëª¨ë“  ê²°ê³¼ëŠ” Google Driveì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. í™˜ê²½ ì„¤ì • & Google Drive ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# í™˜ê²½ í™•ì¸ ë° ëŸ°íƒ€ì„ ë²„ì „ ê¶Œì¥ì‚¬í•­\n",
    "# â˜… ì´ í”„ë¡œì íŠ¸ëŠ” Python 3.10~3.11 + PyTorch 2.x í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤\n",
    "# â˜… Colab ë©”ë‰´: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ ëŸ°íƒ€ì„ ë²„ì „ = 2025.07\n",
    "# ============================================================\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "py_ver = f'{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}'\n",
    "print(f'Python: {py_ver}')\n",
    "\n",
    "# PyTorch/CUDA í™•ì¸\n",
    "try:\n",
    "    import torch\n",
    "    pt_ver = torch.__version__\n",
    "    cuda_ver = torch.version.cuda or 'None'\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    gpu_name = torch.cuda.get_device_name(0) if cuda_available else 'No GPU'\n",
    "    print(f'PyTorch: {pt_ver}')\n",
    "    print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}')\n",
    "    print(f'CUDA ë²„ì „: {cuda_ver}')\n",
    "    print(f'GPU: {gpu_name}')\n",
    "except ImportError:\n",
    "    pt_ver = 'not installed'\n",
    "    cuda_ver = 'unknown'\n",
    "    cuda_available = False\n",
    "    print('PyTorch not installed')\n",
    "\n",
    "print()\n",
    "\n",
    "# í˜¸í™˜ì„± ì²´í¬\n",
    "warnings = []\n",
    "\n",
    "if not cuda_available:\n",
    "    warnings.append(\n",
    "        'GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\\n'\n",
    "        '   -> ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ -> í•˜ë“œì›¨ì–´ ê°€ì†ê¸° -> GPU (A100 ê¶Œì¥)'\n",
    "    )\n",
    "\n",
    "if sys.version_info >= (3, 12):\n",
    "    warnings.append(\n",
    "        f'Python {py_ver} ê°ì§€. ModelScope/basicsrëŠ” Python 3.10~3.11ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\\n'\n",
    "        f'   -> ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ -> ëŸ°íƒ€ì„ ë²„ì „ì„ \"2025.07\"ë¡œ ë³€ê²½ ê¶Œì¥'\n",
    "    )\n",
    "\n",
    "if 'torch' in dir() and hasattr(torch, '__version__'):\n",
    "    pt_major_minor = tuple(int(x) for x in torch.__version__.split('+')[0].split('.')[:2])\n",
    "    if pt_major_minor >= (2, 7):\n",
    "        warnings.append(\n",
    "            f'PyTorch {pt_ver} ê°ì§€. PyTorch3D pre-built wheelì€ 2.4.xê¹Œì§€ë§Œ ì§€ì›.\\n'\n",
    "            f'   -> ì†ŒìŠ¤ ë¹Œë“œë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤ (ìµœì´ˆ ~15ë¶„ ì†Œìš”, ì´í›„ Drive ìºì‹œì—ì„œ < 1ë¶„)'\n",
    "        )\n",
    "\n",
    "if warnings:\n",
    "    print('=' * 60)\n",
    "    print('  í™˜ê²½ ê²½ê³ ')\n",
    "    print('=' * 60)\n",
    "    for i, w in enumerate(warnings, 1):\n",
    "        print(f'  [{i}] {w}')\n",
    "    print('=' * 60)\n",
    "    print()\n",
    "    print('í˜„ì¬ í™˜ê²½ì—ì„œë„ ì‹¤í–‰ì„ ì‹œë„í•˜ì§€ë§Œ, ë¬¸ì œ ë°œìƒ ì‹œ:')\n",
    "    print('  Colab ë©”ë‰´: ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ -> ëŸ°íƒ€ì„ ë²„ì „ -> \"2025.07\"')\n",
    "else:\n",
    "    print('=' * 60)\n",
    "    print('  í™˜ê²½ í˜¸í™˜ì„± í™•ì¸ ì™„ë£Œ!')\n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# GPU í™•ì¸ (ìƒì„¸)\n",
    "!nvidia-smi\n",
    "print()\n",
    "# ninja ë¹Œë“œ ì‹œìŠ¤í…œ ì‚¬ì „ í™•ì¸ (PyTorch3D, nvdiffrast ë¹Œë“œì— í•„ìš”)\n",
    "!which ninja 2>/dev/null && echo 'ninja: OK' || echo 'ninja: Cell 11ì—ì„œ ì„¤ì¹˜ ì˜ˆì •'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "\n",
    "# ============================================================\n",
    "# Google Drive êµ¬ì¡° íƒìƒ‰ ë° ê²½ë¡œ ìë™ ì„¤ì •\n",
    "# ============================================================\n",
    "print('=== Google Drive êµ¬ì¡° íƒìƒ‰ ì¤‘ ===\\n')\n",
    "\n",
    "# 1ë‹¨ê³„: Drive ë£¨íŠ¸ ë‚´ìš© ì¶œë ¥\n",
    "print('[1] Drive ë£¨íŠ¸ ë‚´ìš©:')\n",
    "for item in sorted(os.listdir(DRIVE_ROOT)):\n",
    "    full = os.path.join(DRIVE_ROOT, item)\n",
    "    marker = 'ğŸ“' if os.path.isdir(full) else 'ğŸ“„'\n",
    "    print(f'  {marker} {item}')\n",
    "\n",
    "# ============================================================\n",
    "# 2ë‹¨ê³„: BISTOOL ë£¨íŠ¸ íƒìƒ‰\n",
    "# ============================================================\n",
    "BISTOOL_ROOT = None\n",
    "for candidate in [\n",
    "    os.path.join(DRIVE_ROOT, 'BISTOOL'),\n",
    "    os.path.join(DRIVE_ROOT, 'Project', 'BISTOOL'),\n",
    "    os.path.join(DRIVE_ROOT, 'LAB', 'Project', 'BISTOOL'),\n",
    "]:\n",
    "    if os.path.isdir(candidate):\n",
    "        BISTOOL_ROOT = candidate\n",
    "        break\n",
    "\n",
    "if BISTOOL_ROOT:\n",
    "    print(f'\\n[2] BISTOOL ë£¨íŠ¸: {BISTOOL_ROOT}')\n",
    "    print('    ë‚´ìš©:')\n",
    "    for item in sorted(os.listdir(BISTOOL_ROOT)):\n",
    "        full = os.path.join(BISTOOL_ROOT, item)\n",
    "        marker = 'ğŸ“' if os.path.isdir(full) else 'ğŸ“„'\n",
    "        print(f'      {marker} {item}')\n",
    "else:\n",
    "    print('\\n[2] BISTOOL í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ„ Drive ë£¨íŠ¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.')\n",
    "\n",
    "# ============================================================\n",
    "# 3ë‹¨ê³„: DICOM ë°ì´í„° (DATA_ROOT) íƒìƒ‰\n",
    "# ============================================================\n",
    "DATA_ROOT = None\n",
    "found_zips = []\n",
    "\n",
    "search_dirs = []\n",
    "if BISTOOL_ROOT:\n",
    "    search_dirs.append(os.path.join(BISTOOL_ROOT, 'Data'))\n",
    "    search_dirs.append(BISTOOL_ROOT)\n",
    "search_dirs.extend([\n",
    "    os.path.join(DRIVE_ROOT, 'Data'),\n",
    "    DRIVE_ROOT,\n",
    "])\n",
    "\n",
    "CT_IDS = ['002', '003', '004', '005', '007', '009']\n",
    "\n",
    "for search_dir in search_dirs:\n",
    "    if not os.path.isdir(search_dir):\n",
    "        continue\n",
    "    try:\n",
    "        items = os.listdir(search_dir)\n",
    "    except PermissionError:\n",
    "        continue\n",
    "    for item in items:\n",
    "        if item.endswith('.zip') and item.replace('.zip', '') in CT_IDS:\n",
    "            found_zips.append(os.path.join(search_dir, item))\n",
    "            if DATA_ROOT is None:\n",
    "                DATA_ROOT = search_dir\n",
    "\n",
    "if found_zips:\n",
    "    print(f'\\n[3] DICOM zip íŒŒì¼ ë°œê²¬:')\n",
    "    for z in found_zips:\n",
    "        size_mb = os.path.getsize(z) / (1024**2)\n",
    "        print(f'      {z} ({size_mb:.0f} MB)')\n",
    "\n",
    "if DATA_ROOT:\n",
    "    print(f'\\nâœ… DICOM ë°ì´í„° ë£¨íŠ¸: {DATA_ROOT}')\n",
    "else:\n",
    "    print(f'\\nâš ï¸ DICOM zip íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ DATA_ROOTë¥¼ ì„¤ì •í•˜ì„¸ìš”.')\n",
    "    DATA_ROOT = BISTOOL_ROOT or DRIVE_ROOT\n",
    "\n",
    "# ============================================================\n",
    "# 4ë‹¨ê³„: FaceScape ë°ì´í„° (FACESCAPE_ROOT) íƒìƒ‰\n",
    "# ============================================================\n",
    "FACESCAPE_ROOT = None\n",
    "\n",
    "facescape_candidates = []\n",
    "if BISTOOL_ROOT:\n",
    "    facescape_candidates.extend([\n",
    "        os.path.join(BISTOOL_ROOT, 'FaceScape_Sqsh'),\n",
    "        os.path.join(BISTOOL_ROOT, 'FaceScape'),\n",
    "        os.path.join(BISTOOL_ROOT, 'Facescape'),\n",
    "        os.path.join(BISTOOL_ROOT, 'Data', 'Facescape'),\n",
    "        os.path.join(BISTOOL_ROOT, 'Data', 'FaceScape'),\n",
    "        os.path.join(BISTOOL_ROOT, 'Data', 'FaceScape_Sqsh'),\n",
    "    ])\n",
    "facescape_candidates.extend([\n",
    "    os.path.join(DRIVE_ROOT, 'FaceScape_Sqsh'),\n",
    "    os.path.join(DRIVE_ROOT, 'Facescape'),\n",
    "])\n",
    "\n",
    "for candidate in facescape_candidates:\n",
    "    if os.path.isdir(candidate):\n",
    "        FACESCAPE_ROOT = candidate\n",
    "        break\n",
    "\n",
    "if FACESCAPE_ROOT:\n",
    "    print(f'\\nâœ… FaceScape ë°ì´í„° ë£¨íŠ¸: {FACESCAPE_ROOT}')\n",
    "    print('    ë‚´ìš©:')\n",
    "    for item in sorted(os.listdir(FACESCAPE_ROOT)):\n",
    "        full = os.path.join(FACESCAPE_ROOT, item)\n",
    "        if os.path.isdir(full):\n",
    "            print(f'      ğŸ“ {item}')\n",
    "        else:\n",
    "            size_gb = os.path.getsize(full) / (1024**3)\n",
    "            print(f'      ğŸ“„ {item} ({size_gb:.2f} GB)')\n",
    "else:\n",
    "    print(f'\\nâš ï¸ FaceScape í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')\n",
    "    print('    ìˆ˜ë™ ì„¤ì • ì˜ˆ: FACESCAPE_ROOT = \"/content/drive/MyDrive/BISTOOL/FaceScape_Sqsh\"')\n",
    "\n",
    "print('\\n=== ê²½ë¡œ íƒìƒ‰ ì™„ë£Œ ===')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ë° ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "WORK_DIR = '/content/hrn_workspace'\n",
    "RESULTS_DIR = os.path.join(DRIVE_ROOT, 'BISTOOL_Results')\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, 'HRN'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, 'CodeFormer'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, 'GFPGAN'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, 'Comparison'), exist_ok=True)\n",
    "print(f'ì‘ì—… ë””ë ‰í† ë¦¬: {WORK_DIR}')\n",
    "print(f'ê²°ê³¼ ì €ì¥: {RESULTS_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ì˜ì¡´ì„± ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Google Drive wheel ìºì‹œ â†’ ì¬ì‹œì‘ ì‹œ 15~30ì´ˆ)\n",
    "# ============================================================\n",
    "import torch, sys, os\n",
    "\n",
    "PACKAGES = [\n",
    "    'pydicom', 'scipy', 'scikit-image', 'trimesh', 'matplotlib', 'plotly',\n",
    "    'opencv-python-headless', 'pillow', 'tqdm', 'einops', 'kornia',\n",
    "    'face-alignment', 'lpips', 'torchmetrics'\n",
    "]\n",
    "PKG_STR = ' '.join(PACKAGES)\n",
    "\n",
    "cache_key = f'py3{sys.version_info.minor}_cu{torch.version.cuda.replace(\".\", \"\")}'\n",
    "WHEEL_DIR = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'general_wheels', cache_key)\n",
    "MARKER = os.path.join(WHEEL_DIR, '.complete')\n",
    "\n",
    "if os.path.exists(MARKER):\n",
    "    print(f'ğŸ“¦ ìºì‹œëœ wheelì—ì„œ ì„¤ì¹˜ ì¤‘... ({cache_key})')\n",
    "    ret = os.system(f'pip install -q --no-index --find-links \"{WHEEL_DIR}\" {PKG_STR}')\n",
    "    if ret == 0:\n",
    "        print('âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ ìºì‹œ ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "    else:\n",
    "        print('âš ï¸ ìºì‹œ ì„¤ì¹˜ ì‹¤íŒ¨, PyPIì—ì„œ ì¬ì„¤ì¹˜...')\n",
    "        os.system(f'pip install -q {PKG_STR}')\n",
    "else:\n",
    "    print(f'â³ ìµœì´ˆ ì„¤ì¹˜ + wheel ìºì‹œ ìƒì„± ì¤‘... ({cache_key})')\n",
    "    os.makedirs(WHEEL_DIR, exist_ok=True)\n",
    "    !pip wheel -q -w \"{WHEEL_DIR}\" {PKG_STR}\n",
    "    ret = os.system(f'pip install -q --no-index --find-links \"{WHEEL_DIR}\" {PKG_STR}')\n",
    "    if ret == 0:\n",
    "        with open(MARKER, 'w') as f:\n",
    "            f.write(cache_key)\n",
    "        print('âœ… ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ + ìºì‹œ ì €ì¥ ì™„ë£Œ!')\n",
    "    else:\n",
    "        print('âš ï¸ wheel ì„¤ì¹˜ ì‹¤íŒ¨, PyPIì—ì„œ ì§ì ‘ ì„¤ì¹˜...')\n",
    "        os.system(f'pip install -q {PKG_STR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# HRN ì˜ì¡´ì„±: ModelScope (Google Drive wheel ìºì‹œ)\n",
    "# ============================================================\n",
    "import time\n",
    "_install_start = time.time()\n",
    "\n",
    "PACKAGES_MS = ['modelscope', 'msdatasets', 'datasets']\n",
    "PACKAGES_CV = ['addict', 'yapf', 'onnx']\n",
    "ALL_PKGS = PACKAGES_MS + PACKAGES_CV\n",
    "PKG_STR = ' '.join(ALL_PKGS)\n",
    "\n",
    "cache_key = f'py3{sys.version_info.minor}_cu{torch.version.cuda.replace(\".\", \"\")}'\n",
    "WHEEL_DIR = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'modelscope_wheels', cache_key)\n",
    "MARKER = os.path.join(WHEEL_DIR, '.complete')\n",
    "\n",
    "if os.path.exists(MARKER):\n",
    "    print(f'\\U0001f4e6 ModelScope ìºì‹œì—ì„œ ì„¤ì¹˜ ì¤‘... ({cache_key})')\n",
    "    ret = os.system(f'pip install -q --no-index --find-links \"{WHEEL_DIR}\" {PKG_STR} 2>/dev/null')\n",
    "    if ret == 0:\n",
    "        print('\\u2705 ModelScope ìºì‹œ ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "    else:\n",
    "        print('\\u26a0\\ufe0f ìºì‹œ ì„¤ì¹˜ ì‹¤íŒ¨, PyPIì—ì„œ ì¬ì„¤ì¹˜...')\n",
    "        os.system(f'pip install -q {\" \".join(PACKAGES_MS)}')\n",
    "        os.system(f'pip install -q {\" \".join(PACKAGES_CV)} 2>/dev/null || true')\n",
    "else:\n",
    "    print(f'\\u23f3 ModelScope ìµœì´ˆ ì„¤ì¹˜ + wheel ìºì‹œ ìƒì„± ì¤‘... ({cache_key})')\n",
    "    os.makedirs(WHEEL_DIR, exist_ok=True)\n",
    "    !pip wheel -q -w \"{WHEEL_DIR}\" {' '.join(PACKAGES_MS)}\n",
    "    !pip wheel -q -w \"{WHEEL_DIR}\" {' '.join(PACKAGES_CV)} 2>/dev/null || true\n",
    "    ret = os.system(f'pip install -q --no-index --find-links \"{WHEEL_DIR}\" {PKG_STR} 2>/dev/null')\n",
    "    if ret == 0:\n",
    "        with open(MARKER, 'w') as f:\n",
    "            f.write(cache_key)\n",
    "    else:\n",
    "        os.system(f'pip install -q {\" \".join(PACKAGES_MS)}')\n",
    "        os.system(f'pip install -q {\" \".join(PACKAGES_CV)} 2>/dev/null || true')\n",
    "    print('\\u2705 ModelScope ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "\n",
    "os.environ['PIP_CACHE_DIR'] = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache')\n",
    "_install_elapsed = time.time() - _install_start\n",
    "print(f'   ì„¤ì¹˜ ì‹œê°„: {_install_elapsed:.0f}ì´ˆ')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PyTorch3D ì„¤ì¹˜ (Google Drive ìºì‹œ í™œìš© â†’ ì¬ì‹œì‘ ì‹œ < 1ë¶„)\n",
    "# â˜… wheel íŒŒì¼ëª…ì€ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•ŠìŒ (pip í˜¸í™˜ì„± íƒœê·¸ ë³´ì¡´)\n",
    "# ============================================================\n",
    "import torch, sys, os, glob, shutil\n",
    "\n",
    "pyt_version_str = torch.__version__.split('+')[0].replace('.', '')\n",
    "version_str = ''.join([\n",
    "    f'py3{sys.version_info.minor}',\n",
    "    '_cu', torch.version.cuda.replace('.', ''),\n",
    "    f'_pyt{pyt_version_str}'\n",
    "])\n",
    "print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}, Python: 3.{sys.version_info.minor}')\n",
    "print(f'Wheel key: {version_str}')\n",
    "\n",
    "!pip install -q fvcore iopath\n",
    "\n",
    "# â”€â”€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ â”€â”€\n",
    "try:\n",
    "    import pytorch3d\n",
    "    print(f'âœ… PyTorch3D {pytorch3d.__version__} ì´ë¯¸ ì„¤ì¹˜ë¨ â€” ìŠ¤í‚µ!')\n",
    "except ImportError:\n",
    "    # â”€â”€ Google Drive ìºì‹œ (ë²„ì „ë³„ í•˜ìœ„ í´ë”) â”€â”€\n",
    "    WHEEL_CACHE = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'pytorch3d_wheels', version_str)\n",
    "    os.makedirs(WHEEL_CACHE, exist_ok=True)\n",
    "    cached = sorted(glob.glob(os.path.join(WHEEL_CACHE, 'pytorch3d*.whl')))\n",
    "\n",
    "    installed = False\n",
    "\n",
    "    if cached:\n",
    "        whl = cached[-1]\n",
    "        print(f'ğŸ“¦ ìºì‹œëœ wheel ë°œê²¬: {os.path.basename(whl)}')\n",
    "        ret = os.system(f'pip install -q \"{whl}\"')\n",
    "        if ret == 0:\n",
    "            try:\n",
    "                import pytorch3d\n",
    "                installed = True\n",
    "                print('âœ… PyTorch3D ìºì‹œì—ì„œ ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "            except ImportError:\n",
    "                print('âš ï¸ ìºì‹œ wheel í˜¸í™˜ ì•ˆ ë¨ â€” ì‚­ì œ í›„ ì¬ë¹Œë“œ')\n",
    "                shutil.rmtree(WHEEL_CACHE)\n",
    "                os.makedirs(WHEEL_CACHE, exist_ok=True)\n",
    "        else:\n",
    "            print('âš ï¸ ìºì‹œ wheel ì„¤ì¹˜ ì‹¤íŒ¨ â€” ì‚­ì œ í›„ ì¬ë¹Œë“œ')\n",
    "            shutil.rmtree(WHEEL_CACHE)\n",
    "            os.makedirs(WHEEL_CACHE, exist_ok=True)\n",
    "\n",
    "    if not installed:\n",
    "        print('â³ PyTorch3D ì„¤ì¹˜ ì¤‘... (wheelì„ ë¹Œë“œí•˜ì—¬ Driveì— ìºì‹œí•©ë‹ˆë‹¤)')\n",
    "        WHEEL_TMP = '/tmp/pytorch3d_wheels'\n",
    "        os.makedirs(WHEEL_TMP, exist_ok=True)\n",
    "\n",
    "        # 1) Pre-built wheel ì‹œë„\n",
    "        !pip download --no-deps --no-cache-dir -d {WHEEL_TMP} \\\n",
    "            pytorch3d \\\n",
    "            -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html \\\n",
    "            2>/dev/null\n",
    "\n",
    "        wheels = glob.glob(f'{WHEEL_TMP}/pytorch3d*.whl')\n",
    "        if not wheels:\n",
    "            # 2) ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ\n",
    "            print('Pre-built wheel ì—†ìŒ â†’ ì†ŒìŠ¤ì—ì„œ ë¹Œë“œí•©ë‹ˆë‹¤...')\n",
    "            !pip wheel --no-build-isolation --no-deps \\\n",
    "                \"git+https://github.com/facebookresearch/pytorch3d.git\" \\\n",
    "                -w {WHEEL_TMP}\n",
    "            wheels = glob.glob(f'{WHEEL_TMP}/pytorch3d*.whl')\n",
    "\n",
    "        if wheels:\n",
    "            whl = wheels[0]\n",
    "            ret = os.system(f'pip install -q \"{whl}\"')\n",
    "            if ret == 0:\n",
    "                # Drive ìºì‹œì— ì›ë³¸ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì €ì¥\n",
    "                dest = os.path.join(WHEEL_CACHE, os.path.basename(whl))\n",
    "                shutil.copy2(whl, dest)\n",
    "                print(f'ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {version_str}/{os.path.basename(whl)}')\n",
    "                print('âœ… ë‹¤ìŒ ëŸ°íƒ€ì„ ì¬ì‹œì‘ ì‹œ < 1ë¶„ ë‚´ ì„¤ì¹˜ë©ë‹ˆë‹¤!')\n",
    "            else:\n",
    "                print('âŒ wheel ì„¤ì¹˜ ì‹¤íŒ¨')\n",
    "        else:\n",
    "            print('âŒ wheel ë¹Œë“œ ì‹¤íŒ¨')\n",
    "\n",
    "    # ìµœì¢… í™•ì¸\n",
    "    try:\n",
    "        import pytorch3d\n",
    "        print(f'ğŸ”§ PyTorch3D {pytorch3d.__version__} ì¤€ë¹„ ì™„ë£Œ')\n",
    "    except ImportError:\n",
    "        print('âŒ PyTorch3Dë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# nvdiffrast ì„¤ì¹˜ (Google Drive ìºì‹œ í™œìš© â†’ ì¬ì‹œì‘ ì‹œ ë¹ ë¥¸ ì„¤ì¹˜)\n",
    "# â˜… CUDA_HOME, OpenGL í—¤ë”, C++ ë¹Œë“œ ë„êµ¬ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤\n",
    "# â˜… wheel íŒŒì¼ëª…ì€ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•ŠìŒ (pip í˜¸í™˜ì„± íƒœê·¸ ë³´ì¡´)\n",
    "# ============================================================\n",
    "import subprocess, sys, os, glob, shutil\n",
    "\n",
    "# 1) CUDA í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "cuda_home = None\n",
    "for p in ['/usr/local/cuda', '/usr/local/cuda-12', '/usr/local/cuda-11']:\n",
    "    if os.path.isdir(p):\n",
    "        cuda_home = p\n",
    "        break\n",
    "\n",
    "if cuda_home:\n",
    "    os.environ['CUDA_HOME'] = cuda_home\n",
    "    os.environ['PATH'] = f\"{cuda_home}/bin:{os.environ.get('PATH', '')}\"\n",
    "    print(f'CUDA_HOME={cuda_home}')\n",
    "    !nvcc --version 2>/dev/null | grep release || echo \"nvcc not found in PATH\"\n",
    "else:\n",
    "    print('âš ï¸ CUDA ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ëŸ°íƒ€ì„ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.')\n",
    "\n",
    "# 2) ì‹œìŠ¤í…œ ë¹Œë“œ ì˜ì¡´ì„± (Google Drive .deb ìºì‹œ)\n",
    "import subprocess\n",
    "_ubuntu_ver = subprocess.run(['lsb_release', '-rs'], capture_output=True, text=True).stdout.strip().replace('.', '')\n",
    "DEB_CACHE = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'apt_debs', f'ubuntu{_ubuntu_ver}')\n",
    "DEB_MARKER = os.path.join(DEB_CACHE, '.complete')\n",
    "APT_PKGS = 'build-essential ninja-build freeglut3-dev libgl1-mesa-dev libgles2-mesa-dev libglew-dev libegl1-mesa-dev'\n",
    "\n",
    "if os.path.exists(DEB_MARKER):\n",
    "    print(f'ğŸ“¦ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ìºì‹œì—ì„œ ì„¤ì¹˜ ì¤‘... (ubuntu{_ubuntu_ver})')\n",
    "    !dpkg -i {DEB_CACHE}/*.deb > /dev/null 2>&1 || true\n",
    "    !apt-get -f install -y -qq > /dev/null 2>&1\n",
    "    print('âœ… ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ìºì‹œ ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "else:\n",
    "    print('â³ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ + .deb ìºì‹œ ìƒì„± ì¤‘...')\n",
    "    os.makedirs(DEB_CACHE, exist_ok=True)\n",
    "    !apt-get update -qq > /dev/null 2>&1\n",
    "    !apt-get install -y -qq --download-only {APT_PKGS} > /dev/null 2>&1\n",
    "    !cp /var/cache/apt/archives/*.deb {DEB_CACHE}/ 2>/dev/null || true\n",
    "    !apt-get install -y -qq {APT_PKGS} > /dev/null 2>&1\n",
    "    with open(DEB_MARKER, 'w') as f:\n",
    "        f.write(f'ubuntu{_ubuntu_ver}')\n",
    "    print('âœ… ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ + ìºì‹œ ì €ì¥ ì™„ë£Œ!')\n",
    "\n",
    "# 3) Python ë¹Œë“œ ë„êµ¬\n",
    "!pip install -q ninja cmake setuptools wheel\n",
    "\n",
    "# 4) nvdiffrast ì„¤ì¹˜ (ë²„ì „ë³„ í•˜ìœ„ í´ë” ìºì‹œ)\n",
    "try:\n",
    "    import nvdiffrast.torch\n",
    "    print(f'\\nâœ… nvdiffrast ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!')\n",
    "except ImportError:\n",
    "    # Python + CUDA ë²„ì „ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±\n",
    "    cache_key = f'py3{sys.version_info.minor}_cu{torch.version.cuda.replace(\".\", \"\")}'\n",
    "    WHEEL_CACHE = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'nvdiffrast_wheels', cache_key)\n",
    "    os.makedirs(WHEEL_CACHE, exist_ok=True)\n",
    "    cached = sorted(glob.glob(os.path.join(WHEEL_CACHE, 'nvdiffrast*.whl')))\n",
    "\n",
    "    installed = False\n",
    "\n",
    "    if cached:\n",
    "        whl = cached[-1]\n",
    "        print(f'ğŸ“¦ ìºì‹œëœ wheel ë°œê²¬: {os.path.basename(whl)}')\n",
    "        ret = os.system(f'pip install -q \"{whl}\"')\n",
    "        if ret == 0:\n",
    "            try:\n",
    "                import nvdiffrast.torch\n",
    "                installed = True\n",
    "                print('âœ… nvdiffrast ìºì‹œì—ì„œ ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "            except ImportError:\n",
    "                print('âš ï¸ ìºì‹œ wheel í˜¸í™˜ ì•ˆ ë¨ â€” ì‚­ì œ í›„ ì¬ë¹Œë“œ')\n",
    "                shutil.rmtree(WHEEL_CACHE)\n",
    "                os.makedirs(WHEEL_CACHE, exist_ok=True)\n",
    "        else:\n",
    "            print('âš ï¸ ìºì‹œ wheel ì„¤ì¹˜ ì‹¤íŒ¨ â€” ì‚­ì œ í›„ ì¬ë¹Œë“œ')\n",
    "            shutil.rmtree(WHEEL_CACHE)\n",
    "            os.makedirs(WHEEL_CACHE, exist_ok=True)\n",
    "\n",
    "    if not installed:\n",
    "        print('â³ nvdiffrast ìµœì´ˆ ì„¤ì¹˜ ì¤‘... (Driveì— ìºì‹œ ì €ì¥)')\n",
    "        WHEEL_TMP = '/tmp/nvdiffrast_wheels'\n",
    "        os.makedirs(WHEEL_TMP, exist_ok=True)\n",
    "\n",
    "        !pip wheel --no-build-isolation --no-deps \\\n",
    "            \"git+https://github.com/NVlabs/nvdiffrast.git@v0.3.5\" \\\n",
    "            -w {WHEEL_TMP}\n",
    "\n",
    "        wheels = glob.glob(f'{WHEEL_TMP}/nvdiffrast*.whl')\n",
    "        if wheels:\n",
    "            whl = wheels[0]\n",
    "            ret = os.system(f'pip install -q \"{whl}\"')\n",
    "            if ret == 0:\n",
    "                # Drive ìºì‹œì— ì›ë³¸ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì €ì¥\n",
    "                dest = os.path.join(WHEEL_CACHE, os.path.basename(whl))\n",
    "                shutil.copy2(whl, dest)\n",
    "                print(f'\\nğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}/{os.path.basename(whl)}')\n",
    "            else:\n",
    "                print('âŒ wheel ì„¤ì¹˜ ì‹¤íŒ¨')\n",
    "        else:\n",
    "            # Fallback: ì§ì ‘ ì„¤ì¹˜\n",
    "            print('wheel ë¹Œë“œ ì‹¤íŒ¨ â†’ ì§ì ‘ ì„¤ì¹˜ ì‹œë„...')\n",
    "            !rm -rf /tmp/nvdiffrast\n",
    "            !git clone --quiet --branch v0.3.5 https://github.com/NVlabs/nvdiffrast.git /tmp/nvdiffrast\n",
    "            !pip install --no-build-isolation -q /tmp/nvdiffrast\n",
    "\n",
    "    # ìµœì¢… ê²€ì¦\n",
    "    try:\n",
    "        import nvdiffrast.torch\n",
    "        print(f'\\nâœ… nvdiffrast ì„¤ì¹˜ ì™„ë£Œ!')\n",
    "    except ImportError as e:\n",
    "        print(f'\\nâŒ nvdiffrast ì„¤ì¹˜ ì‹¤íŒ¨: {e}')\n",
    "        print('ëŸ°íƒ€ì„ â†’ GPU í™•ì¸ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ë°ì´í„° íƒìƒ‰ ë° DICOM CT ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage import measure\n",
    "import shutil\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# â˜… í•µì‹¬: Drive zip â†’ Colab ë¡œì»¬ë¡œ ë³µì‚¬ í›„ í•´ì œ (Drive ì§ì ‘ ì½ê¸° 100ë°° ë¹ ë¦„)\n",
    "# ============================================================\n",
    "LOCAL_DATA = '/content/ct_data'\n",
    "os.makedirs(LOCAL_DATA, exist_ok=True)\n",
    "\n",
    "ct_folders = {}\n",
    "for folder_name in ['002', '003', '004', '005', '007', '009']:\n",
    "    local_folder = os.path.join(LOCAL_DATA, folder_name)\n",
    "    zip_path = os.path.join(DATA_ROOT, f'{folder_name}.zip')\n",
    "\n",
    "    # ì´ë¯¸ ë¡œì»¬ì— í•´ì œëìœ¼ë©´ ê±´ë„ˆëœ€\n",
    "    if os.path.exists(local_folder) and len(glob.glob(os.path.join(local_folder, '**/*.dcm'), recursive=True)) > 0:\n",
    "        dcm_count = len(glob.glob(os.path.join(local_folder, '**/*.dcm'), recursive=True))\n",
    "        ct_folders[folder_name] = {'path': local_folder, 'count': dcm_count}\n",
    "        print(f'  [{folder_name}] ì´ë¯¸ ë¡œì»¬ì— ìˆìŒ ({dcm_count} slices)')\n",
    "        continue\n",
    "\n",
    "    # zip íŒŒì¼ì´ Driveì— ìˆìœ¼ë©´ ë¡œì»¬ë¡œ ë³µì‚¬ í›„ í•´ì œ\n",
    "    if os.path.exists(zip_path):\n",
    "        local_zip = os.path.join(LOCAL_DATA, f'{folder_name}.zip')\n",
    "        print(f'  [{folder_name}] Drive â†’ ë¡œì»¬ ë³µì‚¬ ì¤‘...', end=' ', flush=True)\n",
    "        shutil.copy2(zip_path, local_zip)\n",
    "        print('í•´ì œ ì¤‘...', end=' ', flush=True)\n",
    "        with zipfile.ZipFile(local_zip, 'r') as zf:\n",
    "            zf.extractall(LOCAL_DATA)\n",
    "        os.remove(local_zip)  # zip ì‚­ì œ (ë¡œì»¬ ìš©ëŸ‰ ì ˆì•½)\n",
    "\n",
    "        # í•´ì œëœ í´ë” í™•ì¸\n",
    "        if os.path.exists(local_folder):\n",
    "            dcm_count = len(glob.glob(os.path.join(local_folder, '**/*.dcm'), recursive=True))\n",
    "            ct_folders[folder_name] = {'path': local_folder, 'count': dcm_count}\n",
    "            print(f'ì™„ë£Œ! ({dcm_count} slices)')\n",
    "        else:\n",
    "            print(f'í•´ì œëì§€ë§Œ í´ë” êµ¬ì¡° í™•ì¸ í•„ìš”')\n",
    "            # í•˜ìœ„ í´ë” íƒìƒ‰\n",
    "            for sub in os.listdir(LOCAL_DATA):\n",
    "                sub_path = os.path.join(LOCAL_DATA, sub)\n",
    "                if os.path.isdir(sub_path):\n",
    "                    dcms = glob.glob(os.path.join(sub_path, '**/*.dcm'), recursive=True)\n",
    "                    if dcms:\n",
    "                        print(f'    â†’ {sub}: {len(dcms)} dcm files')\n",
    "    else:\n",
    "        # zip ì—†ìœ¼ë©´ í´ë” ì§ì ‘ í™•ì¸ (Drive ë‚´ í•´ì œëœ í´ë”)\n",
    "        drive_folder = os.path.join(DATA_ROOT, folder_name)\n",
    "        if os.path.exists(drive_folder):\n",
    "            print(f'  [{folder_name}] zip ì—†ìŒ, Drive í´ë”ë¥¼ ë¡œì»¬ë¡œ ë³µì‚¬ ì¤‘...')\n",
    "            shutil.copytree(drive_folder, local_folder)\n",
    "            dcm_count = len(glob.glob(os.path.join(local_folder, '**/*.dcm'), recursive=True))\n",
    "            ct_folders[folder_name] = {'path': local_folder, 'count': dcm_count}\n",
    "            print(f'    ì™„ë£Œ! ({dcm_count} slices)')\n",
    "\n",
    "print(f'\\nì´ {len(ct_folders)}ê°œì˜ CT ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ (ë¡œì»¬: {LOCAL_DATA})')\n",
    "\n",
    "# DICOM ë¡œë“œ í•¨ìˆ˜ (ë¡œì»¬ì—ì„œ ì½ìœ¼ë¯€ë¡œ ë¹ ë¦„)\n",
    "def load_dicom_series(folder_path):\n",
    "    \"\"\"DICOM ì‹œë¦¬ì¦ˆë¥¼ ë¡œë“œí•˜ì—¬ 3D ë³¼ë¥¨ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    dcm_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            if f.endswith('.dcm'):\n",
    "                dcm_files.append(os.path.join(root, f))\n",
    "    dcm_files.sort()\n",
    "\n",
    "    slices = [pydicom.dcmread(f) for f in dcm_files]\n",
    "    slices.sort(key=lambda s: float(getattr(s, 'ImagePositionPatient', [0,0,0])[2]))\n",
    "\n",
    "    pixel_spacing = [float(x) for x in slices[0].PixelSpacing]\n",
    "    slice_thickness = float(slices[0].SliceThickness)\n",
    "    spacing = np.array([slice_thickness, pixel_spacing[0], pixel_spacing[1]])\n",
    "\n",
    "    volume = np.stack([s.pixel_array.astype(np.float32) for s in slices])\n",
    "\n",
    "    intercept = float(getattr(slices[0], 'RescaleIntercept', 0))\n",
    "    slope = float(getattr(slices[0], 'RescaleSlope', 1))\n",
    "    volume = volume * slope + intercept\n",
    "\n",
    "    info = {\n",
    "        'patient_id': str(getattr(slices[0], 'PatientID', 'Unknown')),\n",
    "        'modality': str(getattr(slices[0], 'Modality', 'Unknown')),\n",
    "        'rows': slices[0].Rows,\n",
    "        'cols': slices[0].Columns,\n",
    "        'num_slices': len(slices),\n",
    "        'spacing': spacing,\n",
    "    }\n",
    "    return volume, spacing, info"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ì²« ë²ˆì§¸ CT ë°ì´í„° ë¡œë“œ ë° ì‹œê°í™”\n",
    "if not ct_folders:\n",
    "    print('âŒ CT ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ ì…€ì„ í™•ì¸í•˜ì„¸ìš”.')\n",
    "else:\n",
    "    first_folder = list(ct_folders.keys())[0]\n",
    "    print(f'ë¡œë”© ì¤‘: {first_folder}...')\n",
    "    volume, spacing, info = load_dicom_series(ct_folders[first_folder]['path'])\n",
    "    print(f'ë³¼ë¥¨ shape: {volume.shape}')\n",
    "    print(f'Spacing (mm): {spacing}')\n",
    "    print(f'ì •ë³´: {info}')\n",
    "\n",
    "    # 3ì¶• ë‹¨ë©´ ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    mid_z = volume.shape[0] // 2\n",
    "    mid_y = volume.shape[1] // 2\n",
    "    mid_x = volume.shape[2] // 2\n",
    "\n",
    "    axes[0].imshow(volume[mid_z], cmap='gray', vmin=-200, vmax=800)\n",
    "    axes[0].set_title(f'Axial (Z={mid_z})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(volume[:, mid_y, :], cmap='gray', vmin=-200, vmax=800, aspect=spacing[0]/spacing[2])\n",
    "    axes[1].set_title(f'Coronal (Y={mid_y})')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(volume[:, :, mid_x], cmap='gray', vmin=-200, vmax=800, aspect=spacing[0]/spacing[1])\n",
    "    axes[2].set_title(f'Sagittal (X={mid_x})')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.suptitle(f'CT Scan - Patient {first_folder}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'ct_overview_{first_folder}.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CT ë³¼ë¥¨ì—ì„œ ì–¼êµ´ í”¼ë¶€ í‘œë©´ 3D ì¶”ì¶œ (Marching Cubes)\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def extract_face_surface(volume, spacing, threshold=200):\n",
    "    \"\"\"CT ë³¼ë¥¨ì—ì„œ í”¼ë¶€ í‘œë©´ì„ Marching Cubesë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    threshold: HU ê°’ (í”¼ë¶€ í‘œë©´ ~200-300 HU)\n",
    "    \"\"\"\n",
    "    verts, faces, normals, values = measure.marching_cubes(\n",
    "        volume, level=threshold, spacing=tuple(spacing)\n",
    "    )\n",
    "    return verts, faces, normals\n",
    "if 'volume' not in dir() or volume is None:\n",
    "    print('âš ï¸ CT ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cell 13ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.')\n",
    "else:\n",
    "\n",
    "    print('í”¼ë¶€ í‘œë©´ ì¶”ì¶œ ì¤‘ (Marching Cubes)...')\n",
    "    verts, faces, normals = extract_face_surface(volume, spacing, threshold=200)\n",
    "    print(f'Vertices: {verts.shape[0]:,}, Faces: {faces.shape[0]:,}')\n",
    "\n",
    "    # 3D í‘œë©´ ì‹œê°í™” (Poly3DCollection ì‚¬ìš© - plot_trisurfë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„)\n",
    "    print('3D ì‹œê°í™” ë Œë”ë§ ì¤‘...')\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # ì„œë¸Œìƒ˜í”Œë§ (ìµœëŒ€ 20000ê°œ ë©´ìœ¼ë¡œ ì œí•œ â†’ ë¹ ë¥¸ ë Œë”ë§)\n",
    "    max_faces = 20000\n",
    "    step = max(1, len(faces) // max_faces)\n",
    "    sampled_faces = faces[::step]\n",
    "\n",
    "    # Poly3DCollectionìœ¼ë¡œ ë Œë”ë§ (plot_trisurfë³´ë‹¤ 10ë°°+ ë¹ ë¦„)\n",
    "    mesh_verts = verts[sampled_faces]  # (N_faces, 3, 3)\n",
    "    # ì¶• ìˆœì„œ: Z, Y, X â†’ ì‹œê°í™”ìš© X, Y, Zë¡œ ë³€í™˜\n",
    "    mesh_verts_reorder = mesh_verts[:, :, [2, 1, 0]]\n",
    "\n",
    "    poly = Poly3DCollection(mesh_verts_reorder, alpha=0.8, linewidths=0)\n",
    "    poly.set_facecolor('peachpuff')\n",
    "    ax.add_collection3d(poly)\n",
    "\n",
    "    # ì¶• ë²”ìœ„ ì„¤ì •\n",
    "    ax.set_xlim(verts[:, 2].min(), verts[:, 2].max())\n",
    "    ax.set_ylim(verts[:, 1].min(), verts[:, 1].max())\n",
    "    ax.set_zlim(verts[:, 0].min(), verts[:, 0].max())\n",
    "\n",
    "    ax.set_title(f'3D Face Surface - Patient {first_folder}\\n({sampled_faces.shape[0]:,} faces displayed)')\n",
    "    ax.view_init(elev=180, azim=270)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'ct_3d_surface_{first_folder}.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('ì™„ë£Œ!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CT ë³¼ë¥¨ì—ì„œ 2D ì–¼êµ´ ì´ë¯¸ì§€ ë Œë”ë§ (ì •ë©´, ì¸¡ë©´)\n",
    "def render_face_from_ct(volume, spacing, view='frontal'):\n",
    "    \"\"\"CT ë³¼ë¥¨ì—ì„œ Maximum Intensity Projectionìœ¼ë¡œ 2D ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    soft_tissue = np.clip(volume, -100, 400)\n",
    "    soft_tissue = (soft_tissue - (-100)) / (400 - (-100)) * 255\n",
    "    soft_tissue = soft_tissue.astype(np.uint8)\n",
    "\n",
    "    if view == 'frontal':\n",
    "        projection = np.max(soft_tissue[:, :soft_tissue.shape[1]//2, :], axis=1)\n",
    "    elif view == 'lateral':\n",
    "        projection = np.max(soft_tissue[:, :, :soft_tissue.shape[2]//2], axis=2)\n",
    "    else:\n",
    "        projection = np.max(soft_tissue, axis=0)\n",
    "\n",
    "    return projection\n",
    "if 'volume' not in dir() or volume is None:\n",
    "    print('âš ï¸ CT ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cell 13ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.')\n",
    "else:\n",
    "\n",
    "    # ì •ë©´ ë° ì¸¡ë©´ ë Œë”ë§\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    frontal = render_face_from_ct(volume, spacing, 'frontal')\n",
    "    lateral = render_face_from_ct(volume, spacing, 'lateral')\n",
    "    axial = render_face_from_ct(volume, spacing, 'axial')\n",
    "\n",
    "    axes[0].imshow(frontal, cmap='gray')\n",
    "    axes[0].set_title('Frontal MIP')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(lateral, cmap='gray')\n",
    "    axes[1].set_title('Lateral MIP')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(axial, cmap='gray')\n",
    "    axes[2].set_title('Axial MIP')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.suptitle(f'CT-derived Face Projections - Patient {first_folder}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'ct_projections_{first_folder}.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def extract_face_slice_image(volume, spacing):\n",
    "    \"\"\"CT ë³¼ë¥¨ì—ì„œ ì–¼êµ´ ì˜ì—­ì˜ ëŒ€í‘œ ì¶•ë°©í–¥ ìŠ¬ë¼ì´ìŠ¤ ì´ë¯¸ì§€ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    windowed = np.clip(volume, -100, 300)\n",
    "    windowed = ((windowed + 100) / 400 * 255).astype(np.uint8)\n",
    "    num_slices = volume.shape[0]\n",
    "    face_start = int(num_slices * 0.35)\n",
    "    face_end = int(num_slices * 0.75)\n",
    "    face_slices = windowed[face_start:face_end]\n",
    "    tissue_counts = [(i, np.sum(s > 50)) for i, s in enumerate(face_slices)]\n",
    "    tissue_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_indices = [face_start + tissue_counts[i][0] for i in range(min(5, len(tissue_counts)))]\n",
    "    return windowed, best_indices\n",
    "\n",
    "def extract_and_save_face_images(volume, spacing, patient_id, output_dir):\n",
    "    \"\"\"CT ë³¼ë¥¨ì—ì„œ HRN ì…ë ¥ìš© 2D ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_paths = []\n",
    "\n",
    "    soft = np.clip(volume, -100, 400)\n",
    "    soft = ((soft + 100) / 500 * 255).astype(np.uint8)\n",
    "\n",
    "    half_y = soft.shape[1] // 2\n",
    "    frontal_avg = np.mean(soft[:, :half_y, :], axis=1).astype(np.uint8)\n",
    "    frontal_path = os.path.join(output_dir, f'{patient_id}_frontal.png')\n",
    "    cv2.imwrite(frontal_path, frontal_avg)\n",
    "    saved_paths.append(frontal_path)\n",
    "\n",
    "    windowed, best_indices = extract_face_slice_image(volume, spacing)\n",
    "    for i, idx in enumerate(best_indices[:3]):\n",
    "        slice_path = os.path.join(output_dir, f'{patient_id}_axial_{i}.png')\n",
    "        cv2.imwrite(slice_path, windowed[idx])\n",
    "        saved_paths.append(slice_path)\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "# â˜… ë¡œì»¬ì—ì„œ ì²˜ë¦¬ â†’ ê²°ê³¼ë§Œ Driveì— ì €ì¥\n",
    "LOCAL_FACES_DIR = '/content/extracted_faces'  # ë¡œì»¬ (ë¹ ë¦„)\n",
    "FACE_IMAGES_DIR = os.path.join(RESULTS_DIR, 'extracted_faces')  # Drive (ì €ì¥ìš©)\n",
    "\n",
    "all_face_images = {}\n",
    "\n",
    "for folder_name, folder_info in ct_folders.items():\n",
    "    print(f'ì²˜ë¦¬ ì¤‘: Patient {folder_name}...', end=' ', flush=True)\n",
    "    vol, sp, inf = load_dicom_series(folder_info['path'])  # ë¡œì»¬ì—ì„œ ì½ìœ¼ë¯€ë¡œ ë¹ ë¦„\n",
    "    paths = extract_and_save_face_images(vol, sp, folder_name, LOCAL_FACES_DIR)\n",
    "    all_face_images[folder_name] = paths\n",
    "    print(f'ì™„ë£Œ! ({len(paths)} images, shape={vol.shape})')\n",
    "    del vol\n",
    "\n",
    "# ê²°ê³¼ë¥¼ Driveì—ë„ ë³µì‚¬\n",
    "os.makedirs(FACE_IMAGES_DIR, exist_ok=True)\n",
    "shutil.copytree(LOCAL_FACES_DIR, FACE_IMAGES_DIR, dirs_exist_ok=True)\n",
    "\n",
    "print(f'\\nì´ ì¶”ì¶œëœ ì´ë¯¸ì§€ ìˆ˜: {sum(len(v) for v in all_face_images.values())}')\n",
    "print(f'ë¡œì»¬ ì €ì¥: {LOCAL_FACES_DIR}')\n",
    "print(f'Drive ë°±ì—…: {FACE_IMAGES_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ì¶”ì¶œëœ ì–¼êµ´ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (patient_id, paths) in enumerate(all_face_images.items()):\n",
    "    if i >= 6:\n",
    "        break\n",
    "    img = cv2.imread(paths[0], cv2.IMREAD_GRAYSCALE)\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'Patient {patient_id}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('CTì—ì„œ ì¶”ì¶œëœ ì–¼êµ´ ì´ë¯¸ì§€ (ì •ë©´ íˆ¬ì˜)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'extracted_faces_overview.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 4. HRN ëª¨ë¸ - 3D ì–¼êµ´ ë³µì›\n",
    "\n",
    "ë…¼ë¬¸ì˜ í•µì‹¬ ì•„í‚¤í…ì²˜:\n",
    "- **Low-frequency**: 3DMM (BFM) ê¸°ë°˜ coarse mesh ìƒì„±\n",
    "- **Mid-frequency**: Deformation map (64x64, pix2pix) â†’ ìœ¤ê³½ ë””í…Œì¼\n",
    "- **High-frequency**: Displacement map (256x256, pix2pix) â†’ ì£¼ë¦„, ëª¨ê³µ ë“±\n",
    "- **De-retouching Module**: í”¼ë¶€ í…ìŠ¤ì²˜ì™€ ê¸°í•˜í•™ì  ë””í…Œì¼ ë¶„ë¦¬\n",
    "\n",
    "ModelScopeì˜ ì‚¬ì „í•™ìŠµ HRN ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# HRN ëª¨ë¸ ì´ˆê¸°í™” (ModelScope íŒŒì´í”„ë¼ì¸)\n",
    "# â˜… ëª¨ë¸ ê°€ì¤‘ì¹˜(~1GB)ë¥¼ Google Driveì— ìºì‹œ â†’ ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ì¬ë‹¤ìš´ë¡œë“œ ë¶ˆí•„ìš”\n",
    "# ============================================================\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "# ëª¨ë¸ ìºì‹œë¥¼ Google Driveì— ì €ì¥\n",
    "MODEL_CACHE_DIR = os.path.join(DRIVE_ROOT, 'BISTOOL_Models', 'modelscope_cache')\n",
    "os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "os.environ['MODELSCOPE_CACHE'] = MODEL_CACHE_DIR\n",
    "\n",
    "# ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "HRN_MODEL_ID = 'damo/cv_resnet50_face-reconstruction'\n",
    "cached_model_dir = os.path.join(MODEL_CACHE_DIR, 'hub', HRN_MODEL_ID.replace('/', os.sep))\n",
    "if os.path.exists(cached_model_dir):\n",
    "    print(f'âœ… HRN ëª¨ë¸ì´ Driveì— ìºì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤: {cached_model_dir}')\n",
    "    print('   ë‹¤ìš´ë¡œë“œ ì—†ì´ ë°”ë¡œ ë¡œë”©í•©ë‹ˆë‹¤.')\n",
    "else:\n",
    "    print('â¬‡ï¸ HRN ëª¨ë¸ ìµœì´ˆ ë‹¤ìš´ë¡œë“œ ì¤‘ (~1GB, Driveì— ìºì‹œë©ë‹ˆë‹¤)...')\n",
    "\n",
    "import time\n",
    "_load_start = time.time()\n",
    "\n",
    "# v2.0.3 (ìµœì‹ ) ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ v2.0.0-HRN (ì›ë³¸)\n",
    "for _rev in ['v2.0.3', 'v2.0.0-HRN']:\n",
    "    try:\n",
    "        face_reconstruction = pipeline(\n",
    "            Tasks.face_reconstruction,\n",
    "            model=HRN_MODEL_ID,\n",
    "            model_revision=_rev\n",
    "        )\n",
    "        print(f'âœ… HRN ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (revision: {_rev}, {time.time()-_load_start:.1f}ì´ˆ)')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ revision {_rev} ì‹¤íŒ¨: {e}')\n",
    "else:\n",
    "    raise RuntimeError('HRN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ â€” ëª¨ë“  revision ì‹œë„ ì™„ë£Œ')\n",
    "\n",
    "print(f'   ìºì‹œ ìœ„ì¹˜: {MODEL_CACHE_DIR}')\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from modelscope.outputs import OutputKeys\n",
    "import trimesh\n",
    "import json\n",
    "import time\n",
    "\n",
    "def run_hrn_reconstruction(pipeline_model, image_path, output_dir, patient_id):\n",
    "    \"\"\"HRN ëª¨ë¸ë¡œ ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ 3D ì–¼êµ´ ë³µì›ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = pipeline_model(image_path)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    output_info = {'patient_id': patient_id, 'input_image': image_path, 'inference_time': elapsed}\n",
    "    saved_files = {}\n",
    "\n",
    "    # 3D Mesh (OBJ) ì €ì¥\n",
    "    if OutputKeys.OUTPUT in result:\n",
    "        mesh_data = result[OutputKeys.OUTPUT]\n",
    "        if 'mesh' in mesh_data:\n",
    "            obj_path = os.path.join(output_dir, f'{patient_id}_hrn_mesh.obj')\n",
    "            # write_obj ìœ í‹¸ë¦¬í‹° ì‚¬ìš©\n",
    "            try:\n",
    "                from modelscope.models.cv.face_reconstruction.utils import write_obj\n",
    "                write_obj(obj_path, mesh_data['mesh'])\n",
    "                saved_files['mesh'] = obj_path\n",
    "            except Exception:\n",
    "                # ìˆ˜ë™ OBJ ì €ì¥\n",
    "                if hasattr(mesh_data['mesh'], 'vertices'):\n",
    "                    mesh_obj = trimesh.Trimesh(\n",
    "                        vertices=mesh_data['mesh'].vertices,\n",
    "                        faces=mesh_data['mesh'].faces\n",
    "                    )\n",
    "                    mesh_obj.export(obj_path)\n",
    "                    saved_files['mesh'] = obj_path\n",
    "\n",
    "    # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ (vis_image from OUTPUT dict)\n",
    "    output_dict = result.get(OutputKeys.OUTPUT, {})\n",
    "    vis_img = output_dict.get('vis_image') if isinstance(output_dict, dict) else None\n",
    "\n",
    "    # fallback: OUTPUT_IMG (í…ìŠ¤ì²˜ë§µ) ë˜ëŠ” ì…ë ¥ ì´ë¯¸ì§€\n",
    "    if vis_img is None and OutputKeys.OUTPUT_IMG in result:\n",
    "        vis_img = result[OutputKeys.OUTPUT_IMG]\n",
    "    if vis_img is None:\n",
    "        vis_img = cv2.imread(image_path)\n",
    "\n",
    "    if vis_img is not None:\n",
    "        vis_path = os.path.join(output_dir, f'{patient_id}_hrn_visualization.jpg')\n",
    "        if isinstance(vis_img, np.ndarray):\n",
    "            cv2.imwrite(vis_path, vis_img)\n",
    "        else:\n",
    "            with open(vis_path, 'wb') as f:\n",
    "                f.write(vis_img)\n",
    "        saved_files['visualization'] = vis_path\n",
    "\n",
    "    # í…ìŠ¤ì²˜ ì €ì¥\n",
    "    if 'texture' in result.get(OutputKeys.OUTPUT, {}):\n",
    "        tex_path = os.path.join(output_dir, f'{patient_id}_hrn_texture.png')\n",
    "        tex = result[OutputKeys.OUTPUT]['texture']\n",
    "        if isinstance(tex, np.ndarray):\n",
    "            cv2.imwrite(tex_path, tex)\n",
    "        saved_files['texture'] = tex_path\n",
    "\n",
    "    output_info['saved_files'] = saved_files\n",
    "    return result, output_info\n",
    "\n",
    "print('HRN ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# â˜… HRN íŒŒì´í”„ë¼ì¸ ì¤‘ê°„ ê²°ê³¼ ì¶”ì¶œ (Intermediate Results Extraction)\n",
    "# ============================================================\n",
    "# 3DMM Coarse â†’ Mid-Freq Deformation â†’ High-Freq Displacement\n",
    "# ê° ë‹¨ê³„ì˜ ì•Œë² ë„ë§µ, í…ìŠ¤ì²˜ë§µ, ë…¸ë©€ë§µ, ë³€í˜•ë§µ, ë³€ìœ„ë§µ, ë©”ì‰¬ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤\n",
    "# ============================================================\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from modelscope.outputs import OutputKeys\n",
    "\n",
    "if not all_face_images:\n",
    "    print('âŒ ì¶”ì¶œëœ ì–¼êµ´ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. CT ë°ì´í„° ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.')\n",
    "    intermediates = OrderedDict()\n",
    "    test_path = None\n",
    "else:\n",
    "    test_path = list(all_face_images.values())[0][0]\n",
    "    print(f'ì…ë ¥ ì´ë¯¸ì§€: {test_path}\\n')\n",
    "\n",
    "    # â”â”â” STEP 1: íŒŒì´í”„ë¼ì¸ ì¶œë ¥ êµ¬ì¡° ì „ì²´ íƒìƒ‰ â”â”â”\n",
    "    raw_result = face_reconstruction(test_path)\n",
    "\n",
    "    def inspect_obj(obj, name='root', depth=0, max_depth=3):\n",
    "        indent = '  ' * depth\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            print(f'{indent}ndarray  {name}: {obj.shape} [{obj.min():.2f}~{obj.max():.2f}]')\n",
    "        elif isinstance(obj, torch.Tensor):\n",
    "            print(f'{indent}Tensor   {name}: {tuple(obj.shape)}')\n",
    "        elif isinstance(obj, dict):\n",
    "            print(f'{indent}dict     {name}: ({len(obj)} keys)')\n",
    "            for k, v in obj.items():\n",
    "                inspect_obj(v, str(k), depth+1, max_depth)\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            print(f'{indent}{type(obj).__name__:8s} {name}: (len={len(obj)})')\n",
    "            for i, v in enumerate(obj[:3]):\n",
    "                inspect_obj(v, f'[{i}]', depth+1, max_depth)\n",
    "        elif hasattr(obj, 'vertices'):\n",
    "            print(f'{indent}Mesh     {name}: V={obj.vertices.shape} F={obj.faces.shape}')\n",
    "        else:\n",
    "            print(f'{indent}{type(obj).__name__:8s} {name}')\n",
    "\n",
    "    print('â”' * 60)\n",
    "    print('  STEP 1: íŒŒì´í”„ë¼ì¸ ì¶œë ¥ êµ¬ì¡°')\n",
    "    print('â”' * 60)\n",
    "    inspect_obj(raw_result)\n",
    "\n",
    "    # â”â”â” STEP 2: ëª¨ë¸ ë‚´ë¶€ ì„œë¸Œëª¨ë“ˆ êµ¬ì¡° â”â”â”\n",
    "    print('\\n' + 'â”' * 60)\n",
    "    print('  STEP 2: ëª¨ë¸ ì„œë¸Œëª¨ë“ˆ êµ¬ì¡°')\n",
    "    print('â”' * 60)\n",
    "\n",
    "    hrn_model = face_reconstruction.model\n",
    "    print(f'Pipeline model: {type(hrn_model).__name__}')\n",
    "\n",
    "    actual_model = hrn_model\n",
    "    for attr in ['model', 'net', 'network', 'backbone', 'face_model']:\n",
    "        if hasattr(actual_model, attr):\n",
    "            inner = getattr(actual_model, attr)\n",
    "            if hasattr(inner, 'named_modules') or hasattr(inner, 'forward'):\n",
    "                print(f'  â””â†’ .{attr} = {type(inner).__name__}')\n",
    "                actual_model = inner\n",
    "\n",
    "    if hasattr(actual_model, 'named_children'):\n",
    "        print(f'\\n  ì„œë¸Œëª¨ë“ˆ:')\n",
    "        for name, mod in actual_model.named_children():\n",
    "            n_p = sum(p.numel() for p in mod.parameters()) / 1e6\n",
    "            extra = f' ({n_p:.1f}M params)' if n_p > 0.01 else ''\n",
    "            print(f'    {name}: {type(mod).__name__}{extra}')\n",
    "\n",
    "    # â”â”â” STEP 3: Forward Hookìœ¼ë¡œ ì¤‘ê°„ í…ì„œ ìº¡ì²˜ â”â”â”\n",
    "    print('\\n' + 'â”' * 60)\n",
    "    print('  STEP 3: Forward Hook ìº¡ì²˜')\n",
    "    print('â”' * 60)\n",
    "\n",
    "    intermediates = OrderedDict()\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook_fn(module, inp, out):\n",
    "            try:\n",
    "                if isinstance(out, torch.Tensor) and out.dim() >= 2:\n",
    "                    intermediates[name] = out.detach().cpu().numpy()\n",
    "                elif isinstance(out, dict):\n",
    "                    for k, v in out.items():\n",
    "                        if isinstance(v, torch.Tensor) and v.dim() >= 2:\n",
    "                            intermediates[f'{name}/{k}'] = v.detach().cpu().numpy()\n",
    "                        elif isinstance(v, np.ndarray) and v.ndim >= 2:\n",
    "                            intermediates[f'{name}/{k}'] = v\n",
    "                elif isinstance(out, (tuple, list)):\n",
    "                    for i, v in enumerate(out[:5]):\n",
    "                        if isinstance(v, torch.Tensor) and v.dim() >= 2:\n",
    "                            intermediates[f'{name}[{i}]'] = v.detach().cpu().numpy()\n",
    "            except Exception:\n",
    "                pass\n",
    "        return hook_fn\n",
    "\n",
    "    hook_kw = [\n",
    "        '3dmm', 'bfm', 'morphable', 'coarse',\n",
    "        'pix2pix', 'generator', 'unet', 'netG',\n",
    "        'mid', 'deform', 'high', 'displace', 'detail',\n",
    "        'albedo', 'texture', 'normal',\n",
    "        'render', 'recon', 'decode', 'refine',\n",
    "        'retouching', 'de_retouching',\n",
    "    ]\n",
    "\n",
    "    hook_handles = []\n",
    "    target = actual_model if hasattr(actual_model, 'named_modules') else hrn_model\n",
    "\n",
    "    if hasattr(target, 'named_modules'):\n",
    "        for name, mod in target.named_modules():\n",
    "            if any(kw in name.lower() for kw in hook_kw):\n",
    "                hook_handles.append(mod.register_forward_hook(make_hook(name)))\n",
    "                print(f'  Hook: {name} ({type(mod).__name__})')\n",
    "\n",
    "    if not hook_handles and hasattr(target, 'named_children'):\n",
    "        print('  (í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ â†’ ìµœìƒìœ„ ì„œë¸Œëª¨ë“ˆ ì „ì²´ Hook)')\n",
    "        for name, mod in target.named_children():\n",
    "            hook_handles.append(mod.register_forward_hook(make_hook(name)))\n",
    "            print(f'  Hook: {name} ({type(mod).__name__})')\n",
    "\n",
    "    print(f'\\n  Hook {len(hook_handles)}ê°œ ë“±ë¡, ì¶”ë¡  ì‹¤í–‰ ì¤‘...')\n",
    "    hooked_result = face_reconstruction(test_path)\n",
    "\n",
    "    for h in hook_handles:\n",
    "        h.remove()\n",
    "\n",
    "    # â”â”â” STEP 4: OUTPUT dictì—ì„œ ì§ì ‘ ì¶”ì¶œ â”â”â”\n",
    "    output_data = hooked_result.get(OutputKeys.OUTPUT, {})\n",
    "    if isinstance(output_data, dict):\n",
    "        for k, v in output_data.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                intermediates[f'OUTPUT/{k}'] = v\n",
    "            elif isinstance(v, torch.Tensor):\n",
    "                intermediates[f'OUTPUT/{k}'] = v.detach().cpu().numpy()\n",
    "            elif hasattr(v, 'vertices'):\n",
    "                intermediates[f'OUTPUT/{k}'] = v\n",
    "\n",
    "    if OutputKeys.OUTPUT_IMG in hooked_result:\n",
    "        vis = hooked_result[OutputKeys.OUTPUT_IMG]\n",
    "        if isinstance(vis, np.ndarray):\n",
    "            intermediates['OUTPUT_IMG'] = vis\n",
    "\n",
    "    # ê²°ê³¼ ìš”ì•½\n",
    "    print(f'\\n{\"â”\" * 60}')\n",
    "    print(f'  âœ… ì´ {len(intermediates)}ê°œ ì¤‘ê°„ ê²°ê³¼ ìº¡ì²˜ ì™„ë£Œ')\n",
    "    print(f'{\"â”\" * 60}')\n",
    "    for k, v in intermediates.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            print(f'  {k}: shape={v.shape}')\n",
    "        elif hasattr(v, 'vertices'):\n",
    "            print(f'  {k}: Mesh V={v.vertices.shape} F={v.faces.shape}')\n",
    "\n",
    "    print(f'\\nâ†’ ë‹¤ìŒ ì…€ì—ì„œ ì‹œê°í™”í•©ë‹ˆë‹¤.')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# â˜… HRN ì¤‘ê°„ ê²°ê³¼ ì‹œê°í™” (ì•Œë² ë„, í…ìŠ¤ì²˜, ë…¸ë©€, ë³€í˜•/ë³€ìœ„ë§µ, ë©”ì‰¬)\n",
    "# ============================================================\n",
    "from collections import OrderedDict\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "if not intermediates:\n",
    "    print('âš ï¸ ì¤‘ê°„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.')\n",
    "else:\n",
    "    # ---- 2D ë§µ ì‹œê°í™” ----\n",
    "    image_maps = OrderedDict()\n",
    "\n",
    "    if test_path:\n",
    "        inp_img = cv2.imread(test_path)\n",
    "        if inp_img is not None:\n",
    "            image_maps['0_INPUT'] = cv2.cvtColor(inp_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    for k, v in intermediates.items():\n",
    "        if not isinstance(v, np.ndarray):\n",
    "            continue\n",
    "        img = None\n",
    "        if v.ndim == 2 and min(v.shape) > 8:\n",
    "            img = v\n",
    "        elif v.ndim == 3 and v.shape[0] in [1, 3] and v.shape[1] > 8:\n",
    "            img = np.transpose(v, (1, 2, 0)).squeeze()\n",
    "        elif v.ndim == 3 and v.shape[-1] in [1, 3] and v.shape[0] > 8:\n",
    "            img = v.squeeze()\n",
    "        elif v.ndim == 4 and v.shape[0] == 1:\n",
    "            v0 = v[0]\n",
    "            if v0.shape[0] in [1, 3] and v0.shape[1] > 8:\n",
    "                img = np.transpose(v0, (1, 2, 0)).squeeze()\n",
    "            elif v0.shape[-1] in [1, 3] and v0.shape[0] > 8:\n",
    "                img = v0.squeeze()\n",
    "            elif v0.shape[0] > 3 and v0.shape[1] > 8:\n",
    "                img = np.transpose(v0[:3], (1, 2, 0))\n",
    "        if img is not None:\n",
    "            image_maps[k] = img\n",
    "\n",
    "    if 'OUTPUT_IMG' in intermediates:\n",
    "        vis = intermediates['OUTPUT_IMG']\n",
    "        if vis.ndim == 3 and vis.shape[-1] == 3:\n",
    "            image_maps['Z_FINAL_VIS'] = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    n = len(image_maps)\n",
    "    if n > 0:\n",
    "        ncols = min(4, n)\n",
    "        nrows = (n + ncols - 1) // ncols\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4.5 * nrows))\n",
    "        if n == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        else:\n",
    "            axes = np.atleast_2d(axes)\n",
    "\n",
    "        for idx, (name, img) in enumerate(image_maps.items()):\n",
    "            r, c = idx // ncols, idx % ncols\n",
    "            ax = axes[r, c]\n",
    "            img_f = img.astype(np.float64)\n",
    "            if img_f.max() > 1.5:\n",
    "                img_f = np.clip(img_f, 0, 255) / 255.0\n",
    "            elif img_f.min() < -0.5:\n",
    "                img_f = (img_f - img_f.min()) / max(img_f.max() - img_f.min(), 1e-8)\n",
    "            else:\n",
    "                img_f = np.clip(img_f, 0, 1)\n",
    "\n",
    "            if img_f.ndim == 2:\n",
    "                im = ax.imshow(img_f, cmap='viridis')\n",
    "                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            else:\n",
    "                ax.imshow(img_f)\n",
    "            ax.set_title(name.replace('/', '\\n'), fontsize=8, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "\n",
    "        for idx in range(n, nrows * ncols):\n",
    "            axes[idx // ncols, idx % ncols].set_visible(False)\n",
    "\n",
    "        plt.suptitle('HRN Pipeline - ë‹¨ê³„ë³„ ì¤‘ê°„ ê²°ê³¼', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'HRN', 'hrn_intermediate_maps.png'), dpi=200, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f'2D ë§µ {n}ê°œ ì‹œê°í™” ì™„ë£Œ')\n",
    "    else:\n",
    "        print('âš ï¸ ì‹œê°í™” ê°€ëŠ¥í•œ 2D ë§µ ì—†ìŒ. ìœ„ ì…€ì˜ í…ì„œ shapeë¥¼ í™•ì¸í•˜ì„¸ìš”.')\n",
    "\n",
    "    # ---- 3D ë©”ì‰¬ ì‹œê°í™” ----\n",
    "    mesh_items = {k: v for k, v in intermediates.items() if hasattr(v, 'vertices')}\n",
    "    if mesh_items:\n",
    "        n_m = min(len(mesh_items), 3)\n",
    "        fig_m = plt.figure(figsize=(6 * n_m, 5))\n",
    "        for idx, (name, mesh_obj) in enumerate(list(mesh_items.items())[:3]):\n",
    "            ax = fig_m.add_subplot(1, n_m, idx + 1, projection='3d')\n",
    "            vm, fm = mesh_obj.vertices, mesh_obj.faces\n",
    "            step = max(1, len(fm) // 15000)\n",
    "            tris = vm[fm[::step]]\n",
    "            poly = Poly3DCollection(tris, alpha=0.7, linewidths=0)\n",
    "            poly.set_facecolor('peachpuff')\n",
    "            ax.add_collection3d(poly)\n",
    "            ax.set_xlim(vm[:, 0].min(), vm[:, 0].max())\n",
    "            ax.set_ylim(vm[:, 1].min(), vm[:, 1].max())\n",
    "            ax.set_zlim(vm[:, 2].min(), vm[:, 2].max())\n",
    "            ax.set_title(f'{name}\\nV={vm.shape[0]:,} F={fm.shape[0]:,}', fontsize=9)\n",
    "        plt.suptitle('HRN - 3D Mesh (ì¤‘ê°„/ìµœì¢…)', fontsize=13)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'HRN', 'hrn_intermediate_meshes.png'), dpi=150)\n",
    "        plt.show()\n",
    "        print(f'3D ë©”ì‰¬ {len(mesh_items)}ê°œ ì‹œê°í™” ì™„ë£Œ')\n",
    "\n",
    "    print(f'\\nì €ì¥ ìœ„ì¹˜: {os.path.join(RESULTS_DIR, \"HRN\")}/')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# CTì—ì„œ ì¶”ì¶œí•œ ì–¼êµ´ ì´ë¯¸ì§€ë¡œ HRN 3D ë³µì› ìˆ˜í–‰\n",
    "# ============================================================\n",
    "HRN_OUTPUT_DIR = os.path.join(RESULTS_DIR, 'HRN')\n",
    "hrn_results = {}\n",
    "\n",
    "for patient_id, image_paths in all_face_images.items():\n",
    "    # ì •ë©´ ì´ë¯¸ì§€ ì‚¬ìš©\n",
    "    frontal_path = image_paths[0]\n",
    "    print(f'\\n=== Patient {patient_id} HRN ì¶”ë¡  ì¤‘... ===')\n",
    "    print(f'  ì…ë ¥: {frontal_path}')\n",
    "\n",
    "    try:\n",
    "        result, info = run_hrn_reconstruction(\n",
    "            face_reconstruction, frontal_path,\n",
    "            os.path.join(HRN_OUTPUT_DIR, patient_id), patient_id\n",
    "        )\n",
    "        hrn_results[patient_id] = info\n",
    "        print(f'  ì¶”ë¡  ì‹œê°„: {info[\"inference_time\"]:.2f}ì´ˆ')\n",
    "        print(f'  ì €ì¥ëœ íŒŒì¼: {list(info[\"saved_files\"].keys())}')\n",
    "    except Exception as e:\n",
    "        print(f'  ì˜¤ë¥˜ ë°œìƒ: {e}')\n",
    "        hrn_results[patient_id] = {'error': str(e)}\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½ ì €ì¥\n",
    "summary_path = os.path.join(HRN_OUTPUT_DIR, 'hrn_results_summary.json')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hrn_results, f, indent=2, default=str, ensure_ascii=False)\n",
    "print(f'\\nê²°ê³¼ ìš”ì•½ ì €ì¥: {summary_path}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# HRN ê²°ê³¼ ì‹œê°í™”\n",
    "successful_results = {k: v for k, v in hrn_results.items() if 'error' not in v}\n",
    "\n",
    "if successful_results:\n",
    "    n_results = len(successful_results)\n",
    "    fig, axes = plt.subplots(n_results, 3, figsize=(18, 6 * n_results))\n",
    "    if n_results == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "\n",
    "    for i, (patient_id, info) in enumerate(successful_results.items()):\n",
    "        # ì›ë³¸ ì…ë ¥ ì´ë¯¸ì§€\n",
    "        input_img = cv2.imread(info['input_image'])\n",
    "        if input_img is not None:\n",
    "            axes[i, 0].imshow(cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB))\n",
    "        axes[i, 0].set_title(f'Input - Patient {patient_id}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # HRN ì‹œê°í™” ê²°ê³¼\n",
    "        vis_path = info['saved_files'].get('visualization')\n",
    "        if vis_path and os.path.exists(vis_path):\n",
    "            vis_img = cv2.imread(vis_path)\n",
    "            if vis_img is not None:\n",
    "                axes[i, 1].imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))\n",
    "        axes[i, 1].set_title(f'HRN Reconstruction')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # í…ìŠ¤ì²˜ ë§µ\n",
    "        tex_path = info['saved_files'].get('texture')\n",
    "        if tex_path and os.path.exists(tex_path):\n",
    "            tex_img = cv2.imread(tex_path)\n",
    "            if tex_img is not None:\n",
    "                axes[i, 2].imshow(cv2.cvtColor(tex_img, cv2.COLOR_BGR2RGB))\n",
    "        axes[i, 2].set_title(f'Texture Map')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.suptitle('HRN 3D Face Reconstruction Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'HRN', 'hrn_all_results.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('ì„±ê³µì ì¸ HRN ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 5. FaceScape ë°ì´í„° ì‹¤í—˜\n",
    "\n",
    "FaceScape ë°ì´í„°ì…‹ìœ¼ë¡œ HRN ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "- 3D ìŠ¤ìº” ground truthì™€ HRN ë³µì› ê²°ê³¼ ë¹„êµ\n",
    "- Chamfer Distance, Mean Normal Error ì¸¡ì •"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# FaceScape ë°ì´í„° ì¤€ë¹„\n",
    "# â˜… .sqsh (SquashFS) íŒŒì¼ â†’ ë§ˆìš´íŠ¸ë§Œ í•˜ë©´ ì¦‰ì‹œ ì½ê¸° ê°€ëŠ¥ (í•´ì œ ë¶ˆí•„ìš”!)\n",
    "# â˜… zipê³¼ ë‹¬ë¦¬ ë””ìŠ¤í¬ ê³µê°„ ì¶”ê°€ ì‚¬ìš© ì—†ìŒ, ì ‘ê·¼ ì†ë„ë„ ë¹ ë¦„\n",
    "# ============================================================\n",
    "print(f'FaceScape ê²½ë¡œ: {FACESCAPE_ROOT}')\n",
    "LOCAL_FACESCAPE = '/content/facescape_data'\n",
    "\n",
    "\n",
    "if FACESCAPE_ROOT is None or not os.path.exists(FACESCAPE_ROOT):\n",
    "    print('âŒ FaceScape í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')\n",
    "    print('   FACESCAPE_ROOT = \"/content/drive/MyDrive/BISTOOL/FaceScape_Sqsh\"')\n",
    "else:\n",
    "    # squashfs-tools ì„¤ì¹˜ (ë§ˆìš´íŠ¸ì— í•„ìš”)\n",
    "    !apt-get install -y -qq squashfs-tools > /dev/null 2>&1\n",
    "\n",
    "    # Drive ë‚´ FaceScape íŒŒì¼ ëª©ë¡\n",
    "    print(f'\\nğŸ“ FaceScape ë””ë ‰í† ë¦¬ ë‚´ìš©:')\n",
    "    facescape_files = sorted(os.listdir(FACESCAPE_ROOT))\n",
    "    sqsh_files = []\n",
    "    zip_files = []\n",
    "    dir_list = []\n",
    "\n",
    "    for f in facescape_files:\n",
    "        full = os.path.join(FACESCAPE_ROOT, f)\n",
    "        if os.path.isdir(full):\n",
    "            dir_list.append(f)\n",
    "            print(f'  ğŸ“ {f}/')\n",
    "        else:\n",
    "            size_gb = os.path.getsize(full) / (1024**3)\n",
    "            print(f'  ğŸ“„ {f} ({size_gb:.2f} GB)')\n",
    "            if f.endswith('.sqsh'):\n",
    "                sqsh_files.append(f)\n",
    "            elif f.endswith('.zip'):\n",
    "                zip_files.append(f)\n",
    "\n",
    "    # ============================================================\n",
    "    # SquashFS ë§ˆìš´íŠ¸ (í•´ì œ ë¶ˆí•„ìš”, ë””ìŠ¤í¬ ì¶”ê°€ ì‚¬ìš© ì—†ìŒ)\n",
    "    # ============================================================\n",
    "    os.makedirs(LOCAL_FACESCAPE, exist_ok=True)\n",
    "    mounted_paths = []\n",
    "\n",
    "    if sqsh_files:\n",
    "        print(f'\\nâœ… .sqsh íŒŒì¼ {len(sqsh_files)}ê°œ ê°ì§€ â†’ ë§ˆìš´íŠ¸ ë°©ì‹ ì‚¬ìš©')\n",
    "\n",
    "        for sqsh_name in sqsh_files:\n",
    "            sqsh_path = os.path.join(FACESCAPE_ROOT, sqsh_name)\n",
    "            mount_name = sqsh_name.replace('.sqsh', '')\n",
    "            mount_point = os.path.join(LOCAL_FACESCAPE, mount_name)\n",
    "\n",
    "            # ì´ë¯¸ ë§ˆìš´íŠ¸ëìœ¼ë©´ ê±´ë„ˆëœ€\n",
    "            if os.path.ismount(mount_point):\n",
    "                print(f'  [{mount_name}] ì´ë¯¸ ë§ˆìš´íŠ¸ë¨: {mount_point}')\n",
    "                mounted_paths.append(mount_point)\n",
    "                continue\n",
    "\n",
    "            os.makedirs(mount_point, exist_ok=True)\n",
    "\n",
    "            # â˜… Driveì—ì„œ Colab ë¡œì»¬ë¡œ ë³µì‚¬ í›„ ë§ˆìš´íŠ¸ (Driveì—ì„œ ì§ì ‘ ë§ˆìš´íŠ¸í•˜ë©´ ëŠë¦¼)\n",
    "            local_sqsh = f'/content/{sqsh_name}'\n",
    "            if not os.path.exists(local_sqsh):\n",
    "                print(f'  [{mount_name}] Drive â†’ Colab ë³µì‚¬ ì¤‘...', end=' ', flush=True)\n",
    "                import shutil\n",
    "                shutil.copy2(sqsh_path, local_sqsh)\n",
    "                print('ì™„ë£Œ!')\n",
    "\n",
    "            print(f'  [{mount_name}] ë§ˆìš´íŠ¸ ì¤‘...', end=' ', flush=True)\n",
    "            ret = os.system(f'sudo mount -t squashfs -o loop,ro \"{local_sqsh}\" \"{mount_point}\" 2>/dev/null')\n",
    "            if ret == 0:\n",
    "                mounted_paths.append(mount_point)\n",
    "                # ë‚´ìš© í™•ì¸\n",
    "                try:\n",
    "                    items = os.listdir(mount_point)\n",
    "                    print(f'ì™„ë£Œ! ({len(items)}ê°œ í•­ëª©)')\n",
    "                    for item in items[:5]:\n",
    "                        print(f'    {item}')\n",
    "                    if len(items) > 5:\n",
    "                        print(f'    ... ì™¸ {len(items)-5}ê°œ')\n",
    "                except Exception as e:\n",
    "                    print(f'ë§ˆìš´íŠ¸ë¨ (ë‚´ìš© í™•ì¸ ì‹¤íŒ¨: {e})')\n",
    "            else:\n",
    "                # ë§ˆìš´íŠ¸ ì‹¤íŒ¨ ì‹œ unsquashfsë¡œ ì¼ë¶€ë§Œ í•´ì œ\n",
    "                print(f'ë§ˆìš´íŠ¸ ì‹¤íŒ¨, unsquashfsë¡œ í•´ì œ ì‹œë„...')\n",
    "                !unsquashfs -f -d \"{mount_point}\" \"{local_sqsh}\" -e 5 2>/dev/null || \\\n",
    "                    unsquashfs -f -d \"{mount_point}\" \"{local_sqsh}\" 2>/dev/null\n",
    "                if os.listdir(mount_point):\n",
    "                    mounted_paths.append(mount_point)\n",
    "                    print(f'  í•´ì œ ì™„ë£Œ!')\n",
    "\n",
    "    elif zip_files:\n",
    "        # sqshê°€ ì—†ìœ¼ë©´ zip fallback\n",
    "        print(f'\\nâš ï¸ .sqsh ì—†ìŒ, .zip fallback ({len(zip_files)}ê°œ)')\n",
    "        import zipfile\n",
    "        for z in zip_files[:1]:\n",
    "            zip_path = os.path.join(FACESCAPE_ROOT, z)\n",
    "            local_zip = f'/content/{z}'\n",
    "            print(f'  [{z}] Drive â†’ Colab ë³µì‚¬ ì¤‘...')\n",
    "            shutil.copy2(zip_path, local_zip)\n",
    "            print(f'  Subject 1~5ë§Œ í•´ì œ ì¤‘...')\n",
    "            with zipfile.ZipFile(local_zip, 'r') as zf:\n",
    "                for name in zf.namelist():\n",
    "                    parts = name.split('/')\n",
    "                    if len(parts) > 1 and parts[0].isdigit() and int(parts[0]) <= 5:\n",
    "                        try:\n",
    "                            zf.extract(name, LOCAL_FACESCAPE)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            os.remove(local_zip)\n",
    "            mounted_paths.append(LOCAL_FACESCAPE)\n",
    "            print(f'  ì™„ë£Œ!')\n",
    "\n",
    "    elif dir_list:\n",
    "        print(f'\\ní´ë” ê°ì§€ â†’ ë¡œì»¬ ë³µì‚¬ ì¤‘...')\n",
    "        for d in dir_list[:1]:\n",
    "            src = os.path.join(FACESCAPE_ROOT, d)\n",
    "            dst = os.path.join(LOCAL_FACESCAPE, d)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copytree(src, dst)\n",
    "            mounted_paths.append(dst)\n",
    "            print(f'  {d} â†’ ë³µì‚¬ ì™„ë£Œ')\n",
    "\n",
    "    # ì´ë¯¸ì§€ íŒŒì¼ íƒìƒ‰\n",
    "    local_images = []\n",
    "    for mp in mounted_paths:\n",
    "        for root, dirs, files in os.walk(mp):\n",
    "            for f in files:\n",
    "                if f.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    local_images.append(os.path.join(root, f))\n",
    "            if len(local_images) > 100:\n",
    "                break\n",
    "\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'FaceScape ì´ë¯¸ì§€: {len(local_images)}ê°œ ë°œê²¬')\n",
    "    if local_images:\n",
    "        print(f'  ì˜ˆ: {local_images[0]}')\n",
    "    print(f'ë§ˆìš´íŠ¸ ê²½ë¡œ: {mounted_paths}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# FaceScape ì´ë¯¸ì§€ë¡œ HRN ë³µì› ë° í‰ê°€ (â˜… ë¡œì»¬ì—ì„œ ì²˜ë¦¬)\n",
    "facescape_hrn_results = {}  # í•­ìƒ ì´ˆê¸°í™” (ì´í›„ ì…€ì—ì„œ ì°¸ì¡°)\n",
    "\n",
    "facescape_images = []\n",
    "for root, dirs, files in os.walk(LOCAL_FACESCAPE):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            facescape_images.append(os.path.join(root, f))\n",
    "\n",
    "print(f'FaceScape ì´ë¯¸ì§€ {len(facescape_images)}ê°œ ë°œê²¬ (ë¡œì»¬: {LOCAL_FACESCAPE})')\n",
    "\n",
    "if len(facescape_images) == 0:\n",
    "    print('âš ï¸ FaceScape ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. FaceScape ë°ì´í„° ì¤€ë¹„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.')\n",
    "else:\n",
    "    # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€ë¡œ HRN í…ŒìŠ¤íŠ¸\n",
    "    FACESCAPE_HRN_DIR = os.path.join(RESULTS_DIR, 'HRN', 'facescape')\n",
    "    os.makedirs(FACESCAPE_HRN_DIR, exist_ok=True)\n",
    "\n",
    "    test_images = facescape_images[:10]\n",
    "\n",
    "    for i, img_path in enumerate(test_images):\n",
    "        subject_id = f'facescape_{i:03d}'\n",
    "        print(f'[{i+1}/{len(test_images)}] {os.path.basename(img_path)}...')\n",
    "\n",
    "        try:\n",
    "            result, info = run_hrn_reconstruction(\n",
    "                face_reconstruction, img_path,\n",
    "                os.path.join(FACESCAPE_HRN_DIR, subject_id), subject_id\n",
    "            )\n",
    "            facescape_hrn_results[subject_id] = info\n",
    "            print(f'  ì™„ë£Œ ({info[\"inference_time\"]:.2f}s)')\n",
    "        except Exception as e:\n",
    "            print(f'  ì˜¤ë¥˜: {e}')\n",
    "            facescape_hrn_results[subject_id] = {'error': str(e)}\n",
    "\n",
    "    print(f'\\nFaceScape HRN ê²°ê³¼: {len([v for v in facescape_hrn_results.values() if \"error\" not in v])} ì„±ê³µ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 6. ìµœì‹  ê¸°ë²• ë¹„êµ - CodeFormer & GFPGAN\n",
    "\n",
    "### ë¹„êµ ëŒ€ìƒ ê¸°ë²•:\n",
    "1. **CodeFormer** (NeurIPS 2022): Codebook ê¸°ë°˜ Transformerë¡œ ì–¼êµ´ ë³µì›. ì—´í™”ëœ ì–¼êµ´ì„ ê³ í’ˆì§ˆë¡œ ë³µì›.\n",
    "2. **GFPGAN v1.4** (CVPR 2021): GAN ê¸°ë°˜ ì–¼êµ´ ë³µì›. GFP (Generative Facial Prior) í™œìš©.\n",
    "\n",
    "ì´ ë°©ë²•ë“¤ì€ 2D ì–¼êµ´ ë³µì›(restoration) ê¸°ë²•ìœ¼ë¡œ, HRNì˜ 3D ë³µì›ê³¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë¹„êµí•©ë‹ˆë‹¤:\n",
    "- **HRN**: 2D â†’ 3D ê¸°í•˜í•™ì  ë³µì› (mesh, texture)\n",
    "- **CodeFormer/GFPGAN**: 2D â†’ 2D í’ˆì§ˆ ë³µì› (í•´ìƒë„, ë””í…Œì¼ í–¥ìƒ)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 6-1. CodeFormer ì„¤ì¹˜ ë° ì„¤ì • (Google Drive ìºì‹œ)\n",
    "# \\u2605 basicsrëŠ” site-packagesì— ì„¤ì¹˜í•˜ì§€ ì•ŠìŒ (GFPGANê³¼ ì¶©ëŒ ë°©ì§€)\n",
    "# \\u2605 ì¶”ë¡  ì‹œ cd CodeFormerë¡œ ì‹¤í–‰ â†’ ë²ˆë“¤ basicsr ìë™ ì‚¬ìš©\n",
    "# ============================================================\n",
    "CODEFORMER_DIR = os.path.join(WORK_DIR, 'CodeFormer')\n",
    "\n",
    "# Drive ìºì‹œ ê²½ë¡œ\n",
    "CF_TAR = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'codeformer_cache', 'codeformer_full.tar.gz')\n",
    "CF_TAR_MARKER = CF_TAR + '.complete'\n",
    "cache_key = f'py3{sys.version_info.minor}_cu{torch.version.cuda.replace(\".\", \"\")}'\n",
    "CF_WHEEL_DIR = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'codeformer_wheels', cache_key)\n",
    "CF_WHEEL_MARKER = os.path.join(CF_WHEEL_DIR, '.complete')\n",
    "\n",
    "if os.path.exists(CODEFORMER_DIR) and os.path.exists(os.path.join(CODEFORMER_DIR, 'inference_codeformer.py')):\n",
    "    print(f'CodeFormer ì´ë¯¸ ì„¤ì¹˜ë¨: {CODEFORMER_DIR}')\n",
    "elif os.path.exists(CF_TAR) and os.path.exists(CF_TAR_MARKER):\n",
    "    # Drive ìºì‹œì—ì„œ ë³µì› (30~60ì´ˆ)\n",
    "    print('CodeFormer ìºì‹œì—ì„œ ë³µì› ì¤‘...')\n",
    "    !tar xzf \"{CF_TAR}\" -C {os.path.dirname(CODEFORMER_DIR)}\n",
    "    # pip ì˜ì¡´ì„± ë³µì› (basicsr ì œì™¸ - GFPGANì´ PyPIì—ì„œ ì„¤ì¹˜)\n",
    "    if os.path.exists(CF_WHEEL_MARKER):\n",
    "        !pip install -q --no-index --find-links \"{CF_WHEEL_DIR}\" \\\n",
    "            lpips opencv-python scikit-image scipy pillow pyyaml tqdm addict 2>/dev/null || true\n",
    "    else:\n",
    "        !pip install -q lpips opencv-python scikit-image scipy pillow pyyaml tqdm addict 2>/dev/null || true\n",
    "    print('\\u2705 CodeFormer ìºì‹œ ë³µì› ì™„ë£Œ!')\n",
    "else:\n",
    "    # ìµœì´ˆ ì„¤ì¹˜ + Driveì— ìºì‹œ ì €ì¥\n",
    "    print('CodeFormer ìµœì´ˆ ì„¤ì¹˜ ì¤‘...')\n",
    "    !git clone https://github.com/sczhou/CodeFormer.git {CODEFORMER_DIR}\n",
    "\n",
    "    # pip deps wheel ìºì‹œ (basicsr, tb-nightly ì œì™¸)\n",
    "    CF_DEPS = 'lpips opencv-python scikit-image scipy pillow pyyaml tqdm addict'\n",
    "    os.makedirs(CF_WHEEL_DIR, exist_ok=True)\n",
    "    !pip wheel -q -w \"{CF_WHEEL_DIR}\" {CF_DEPS} 2>/dev/null || true\n",
    "    !pip install -q {CF_DEPS} 2>/dev/null || true\n",
    "    with open(CF_WHEEL_MARKER, 'w') as f:\n",
    "        f.write('ok')\n",
    "\n",
    "    # pretrained ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "    !cd {CODEFORMER_DIR} && python scripts/download_pretrained_models.py facelib\n",
    "    !cd {CODEFORMER_DIR} && python scripts/download_pretrained_models.py CodeFormer\n",
    "\n",
    "    # tarballë¡œ Driveì— ì €ì¥\n",
    "    os.makedirs(os.path.dirname(CF_TAR), exist_ok=True)\n",
    "    !tar czf \"{CF_TAR}\" -C {os.path.dirname(CODEFORMER_DIR)} CodeFormer\n",
    "    with open(CF_TAR_MARKER, 'w') as f:\n",
    "        f.write('ok')\n",
    "    print('\\u2705 CodeFormer ì„¤ì¹˜ + Drive ìºì‹œ ì €ì¥ ì™„ë£Œ!')\n",
    "\n",
    "print(f'CodeFormer ì¤€ë¹„ ì™„ë£Œ: {CODEFORMER_DIR}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 6-2. GFPGAN ì„¤ì¹˜ ë° ì„¤ì • (Google Drive ìºì‹œ)\n",
    "# \\u2605 basicsr, facexlib, realesrganë¥¼ ë¨¼ì € ì„¤ì¹˜ í›„ gfpgan ì„¤ì¹˜\n",
    "# ============================================================\n",
    "cache_key = f'py3{sys.version_info.minor}_cu{torch.version.cuda.replace(\".\", \"\")}'\n",
    "GFPGAN_WHEEL_DIR = os.path.join(DRIVE_ROOT, 'BISTOOL_pip_cache', 'gfpgan_wheels', cache_key)\n",
    "GFPGAN_WHEEL_MARKER = os.path.join(GFPGAN_WHEEL_DIR, '.complete')\n",
    "\n",
    "# 1) pip install (wheel ìºì‹œ)\n",
    "GFPGAN_PKGS = 'basicsr facexlib gfpgan realesrgan'\n",
    "if os.path.exists(GFPGAN_WHEEL_MARKER):\n",
    "    print(f'GFPGAN ìºì‹œì—ì„œ ì„¤ì¹˜ ì¤‘... ({cache_key})')\n",
    "    ret = os.system(f'pip install -q --no-index --find-links \"{GFPGAN_WHEEL_DIR}\" {GFPGAN_PKGS}')\n",
    "    if ret != 0:\n",
    "        print('\\u26a0\\ufe0f ìºì‹œ ì‹¤íŒ¨, PyPIì—ì„œ ì¬ì„¤ì¹˜...')\n",
    "        os.system(f'pip install -q {GFPGAN_PKGS}')\n",
    "else:\n",
    "    print('GFPGAN ìµœì´ˆ ì„¤ì¹˜ ì¤‘...')\n",
    "    os.makedirs(GFPGAN_WHEEL_DIR, exist_ok=True)\n",
    "    !pip wheel -q -w \"{GFPGAN_WHEEL_DIR}\" {GFPGAN_PKGS} 2>/dev/null || true\n",
    "    !pip install -q basicsr facexlib\n",
    "    !pip install -q gfpgan realesrgan\n",
    "    with open(GFPGAN_WHEEL_MARKER, 'w') as f:\n",
    "        f.write('ok')\n",
    "\n",
    "# ì„¤ì¹˜ í™•ì¸\n",
    "try:\n",
    "    import gfpgan\n",
    "    print('\\u2705 GFPGAN ì„¤ì¹˜ í™•ì¸ ì™„ë£Œ')\n",
    "except ImportError as e:\n",
    "    print(f'\\u26a0\\ufe0f GFPGAN import ì‹¤íŒ¨: {e}')\n",
    "    print('   pip install gfpganìœ¼ë¡œ ì§ì ‘ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...')\n",
    "    !pip install -q gfpgan\n",
    "\n",
    "# 2) ëª¨ë¸ íŒŒì¼ (Driveì— ì˜êµ¬ ìºì‹œ)\n",
    "GFPGAN_MODEL_CACHE = os.path.join(DRIVE_ROOT, 'BISTOOL_Models', 'GFPGANv1.4.pth')\n",
    "GFPGAN_MODEL_DIR = os.path.join(WORK_DIR, 'gfpgan_models')\n",
    "os.makedirs(GFPGAN_MODEL_DIR, exist_ok=True)\n",
    "gfpgan_model_path = os.path.join(GFPGAN_MODEL_DIR, 'GFPGANv1.4.pth')\n",
    "\n",
    "if os.path.exists(gfpgan_model_path):\n",
    "    print(f'GFPGAN ëª¨ë¸ ì´ë¯¸ ë¡œì»¬ì— ìˆìŒ')\n",
    "elif os.path.exists(GFPGAN_MODEL_CACHE):\n",
    "    print('GFPGAN ëª¨ë¸ Drive ìºì‹œì—ì„œ ë³µì‚¬ ì¤‘...')\n",
    "    shutil.copy2(GFPGAN_MODEL_CACHE, gfpgan_model_path)\n",
    "    print('GFPGAN ëª¨ë¸ ë³µì› ì™„ë£Œ!')\n",
    "else:\n",
    "    print('GFPGAN ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...')\n",
    "    !wget -q -O {gfpgan_model_path} https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/GFPGANv1.4.pth\n",
    "    os.makedirs(os.path.dirname(GFPGAN_MODEL_CACHE), exist_ok=True)\n",
    "    shutil.copy2(gfpgan_model_path, GFPGAN_MODEL_CACHE)\n",
    "    print('GFPGAN ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + Drive ìºì‹œ ì €ì¥ ì™„ë£Œ!')\n",
    "\n",
    "print('\\u2705 GFPGAN ì¤€ë¹„ ì™„ë£Œ!')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 6-3. CodeFormerë¡œ ì–¼êµ´ ë³µì› ì‹¤í–‰\n",
    "# \\u2605 cd CodeFormerë¡œ ì‹¤í–‰ â†’ Pythonì´ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.path[0]ì— ì¶”ê°€\n",
    "# \\u2605 â†’ ë²ˆë“¤ basicsrê³¼ facelib ìë™ ì‚¬ìš© (GFPGANê³¼ ì¶©ëŒ ì—†ìŒ)\n",
    "# ============================================================\n",
    "CODEFORMER_RESULTS_DIR = os.path.join(RESULTS_DIR, 'CodeFormer')\n",
    "os.makedirs(CODEFORMER_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# realesrgan ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\n",
    "try:\n",
    "    import realesrgan\n",
    "    _bg_upsampler = '--bg_upsampler realesrgan'\n",
    "except ImportError:\n",
    "    _bg_upsampler = ''\n",
    "    print('  (realesrgan ë¯¸ì„¤ì¹˜ -> ë°°ê²½ ì—…ìƒ˜í”Œë§ ê±´ë„ˆëœ€)')\n",
    "\n",
    "# CodeFormer ì¶”ë¡  ì‹¤í–‰\n",
    "codeformer_results = {}\n",
    "\n",
    "# CTì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€ + FaceScape ì´ë¯¸ì§€ ëª¨ë‘ ì²˜ë¦¬\n",
    "all_test_images = {}\n",
    "\n",
    "# CT ì´ë¯¸ì§€\n",
    "if 'all_face_images' in dir() and all_face_images:\n",
    "    for patient_id, paths in all_face_images.items():\n",
    "        all_test_images[f'ct_{patient_id}'] = paths[0]\n",
    "\n",
    "# FaceScape ì´ë¯¸ì§€\n",
    "if 'facescape_hrn_results' in dir() and facescape_hrn_results:\n",
    "    for subject_id, info in facescape_hrn_results.items():\n",
    "        if isinstance(info, dict) and 'error' not in info:\n",
    "            all_test_images[subject_id] = info.get('input_image', '')\n",
    "\n",
    "if not all_test_images:\n",
    "    print('\\u26a0\\ufe0f ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. CT ë°ì´í„° ë˜ëŠ” FaceScape ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.')\n",
    "else:\n",
    "    for img_id, img_path in all_test_images.items():\n",
    "        output_path = os.path.join(CODEFORMER_RESULTS_DIR, f'{img_id}_codeformer.png')\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            codeformer_results[img_id] = output_path\n",
    "            continue\n",
    "\n",
    "        print(f'CodeFormer ì²˜ë¦¬ ì¤‘: {img_id}...')\n",
    "        try:\n",
    "            !cd {CODEFORMER_DIR} && python inference_codeformer.py \\\n",
    "                -w 0.7 \\\n",
    "                --input_path {img_path} \\\n",
    "                --output_path {os.path.join(CODEFORMER_RESULTS_DIR, img_id)} \\\n",
    "                {_bg_upsampler} \\\n",
    "                --face_upsample 2>/dev/null\n",
    "\n",
    "            # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°\n",
    "            result_dir = os.path.join(CODEFORMER_RESULTS_DIR, img_id, 'final_results')\n",
    "            if os.path.exists(result_dir):\n",
    "                result_files = glob.glob(os.path.join(result_dir, '*.png'))\n",
    "                if result_files:\n",
    "                    shutil.copy(result_files[0], output_path)\n",
    "                    codeformer_results[img_id] = output_path\n",
    "                    print(f'  ì™„ë£Œ: {output_path}')\n",
    "                else:\n",
    "                    print(f'  ê²°ê³¼ íŒŒì¼ ì—†ìŒ')\n",
    "            else:\n",
    "                print(f'  ê²°ê³¼ ë””ë ‰í† ë¦¬ ì—†ìŒ')\n",
    "        except Exception as e:\n",
    "            print(f'  ì˜¤ë¥˜: {e}')\n",
    "\n",
    "    print(f'\\nCodeFormer ì™„ë£Œ: {len(codeformer_results)} ì´ë¯¸ì§€ ì²˜ë¦¬ë¨')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 6-4. GFPGANìœ¼ë¡œ ì–¼êµ´ ë³µì› ì‹¤í–‰\n",
    "# ============================================================\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "# GFPGAN ëª¨ë¸ ë¡œë“œ\n",
    "gfpganer = GFPGANer(\n",
    "    model_path=gfpgan_model_path,\n",
    "    upscale=2,\n",
    "    arch='clean',\n",
    "    channel_multiplier=2,\n",
    "    bg_upsampler=None\n",
    ")\n",
    "\n",
    "GFPGAN_RESULTS_DIR = os.path.join(RESULTS_DIR, 'GFPGAN')\n",
    "os.makedirs(GFPGAN_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "gfpgan_results = {}\n",
    "\n",
    "if not all_test_images:\n",
    "    print('\\u26a0\\ufe0f ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.')\n",
    "else:\n",
    "    for img_id, img_path in all_test_images.items():\n",
    "        output_path = os.path.join(GFPGAN_RESULTS_DIR, f'{img_id}_gfpgan.png')\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            gfpgan_results[img_id] = output_path\n",
    "            continue\n",
    "\n",
    "        print(f'GFPGAN ì²˜ë¦¬ ì¤‘: {img_id}...')\n",
    "        try:\n",
    "            input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "            if input_img is None:\n",
    "                gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if gray is not None:\n",
    "                    input_img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "                else:\n",
    "                    print(f'  ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}')\n",
    "                    continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            _, _, restored_img = gfpganer.enhance(\n",
    "                input_img, has_aligned=False, only_center_face=False, paste_back=True\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            cv2.imwrite(output_path, restored_img)\n",
    "            gfpgan_results[img_id] = output_path\n",
    "            print(f'  ì™„ë£Œ ({elapsed:.2f}s): {output_path}')\n",
    "        except Exception as e:\n",
    "            print(f'  ì˜¤ë¥˜: {e}')\n",
    "\n",
    "    print(f'\\nGFPGAN ì™„ë£Œ: {len(gfpgan_results)} ì´ë¯¸ì§€ ì²˜ë¦¬ë¨')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 7. ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "### í‰ê°€ ë©”íŠ¸ë¦­:\n",
    "- **PSNR** (Peak Signal-to-Noise Ratio): ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "- **SSIM** (Structural Similarity Index): 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ\n",
    "- **LPIPS** (Learned Perceptual Image Patch Similarity): ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "- **ì¶”ë¡  ì‹œê°„**: ì‹¤ìš©ì„± ë¹„êµ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 7-1. ì •ëŸ‰ì  í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "# ============================================================\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import lpips\n",
    "import pandas as pd\n",
    "\n",
    "# LPIPS ëª¨ë¸ ë¡œë“œ\n",
    "lpips_model = lpips.LPIPS(net='alex').cuda()\n",
    "\n",
    "def compute_metrics(img1_path, img2_path):\n",
    "    \"\"\"ë‘ ì´ë¯¸ì§€ ê°„ì˜ PSNR, SSIM, LPIPSë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "\n",
    "    if img1 is None or img2 is None:\n",
    "        return {'psnr': None, 'ssim': None, 'lpips': None}\n",
    "\n",
    "    # í¬ê¸° ë§ì¶”ê¸°\n",
    "    h = min(img1.shape[0], img2.shape[0])\n",
    "    w = min(img1.shape[1], img2.shape[1])\n",
    "    img1 = cv2.resize(img1, (w, h))\n",
    "    img2 = cv2.resize(img2, (w, h))\n",
    "\n",
    "    # PSNR\n",
    "    psnr_val = psnr(img1, img2)\n",
    "\n",
    "    # SSIM\n",
    "    ssim_val = ssim(img1, img2, channel_axis=2)\n",
    "\n",
    "    # LPIPS\n",
    "    t1 = torch.from_numpy(img1).permute(2, 0, 1).unsqueeze(0).float() / 127.5 - 1\n",
    "    t2 = torch.from_numpy(img2).permute(2, 0, 1).unsqueeze(0).float() / 127.5 - 1\n",
    "    with torch.no_grad():\n",
    "        lpips_val = lpips_model(t1.cuda(), t2.cuda()).item()\n",
    "\n",
    "    return {'psnr': psnr_val, 'ssim': ssim_val, 'lpips': lpips_val}\n",
    "\n",
    "print('í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 7-2. ëª¨ë“  ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ì™€ ë¹„êµ\n",
    "# ============================================================\n",
    "comparison_data = []\n",
    "\n",
    "for img_id, original_path in all_test_images.items():\n",
    "    row = {'image_id': img_id}\n",
    "\n",
    "    # HRN ì‹œê°í™” ê²°ê³¼ì™€ ë¹„êµ (2D rendered face)\n",
    "    hrn_info = hrn_results.get(img_id.replace('ct_', ''), facescape_hrn_results.get(img_id, {}))\n",
    "    hrn_vis_path = hrn_info.get('saved_files', {}).get('visualization') if isinstance(hrn_info, dict) else None\n",
    "\n",
    "    if hrn_vis_path and os.path.exists(hrn_vis_path):\n",
    "        metrics = compute_metrics(original_path, hrn_vis_path)\n",
    "        row['hrn_psnr'] = metrics['psnr']\n",
    "        row['hrn_ssim'] = metrics['ssim']\n",
    "        row['hrn_lpips'] = metrics['lpips']\n",
    "        row['hrn_time'] = hrn_info.get('inference_time', None)\n",
    "\n",
    "    # CodeFormer ê²°ê³¼ì™€ ë¹„êµ\n",
    "    cf_path = codeformer_results.get(img_id)\n",
    "    if cf_path and os.path.exists(cf_path):\n",
    "        metrics = compute_metrics(original_path, cf_path)\n",
    "        row['codeformer_psnr'] = metrics['psnr']\n",
    "        row['codeformer_ssim'] = metrics['ssim']\n",
    "        row['codeformer_lpips'] = metrics['lpips']\n",
    "\n",
    "    # GFPGAN ê²°ê³¼ì™€ ë¹„êµ\n",
    "    gf_path = gfpgan_results.get(img_id)\n",
    "    if gf_path and os.path.exists(gf_path):\n",
    "        metrics = compute_metrics(original_path, gf_path)\n",
    "        row['gfpgan_psnr'] = metrics['psnr']\n",
    "        row['gfpgan_ssim'] = metrics['ssim']\n",
    "        row['gfpgan_lpips'] = metrics['lpips']\n",
    "\n",
    "    comparison_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print('=== ì •ëŸ‰ì  ë¹„êµ ê²°ê³¼ ===')\n",
    "print(df.to_string(index=False))\n",
    "df.to_csv(os.path.join(RESULTS_DIR, 'Comparison', 'quantitative_results.csv'), index=False)\n",
    "print(f'\\nì €ì¥: {os.path.join(RESULTS_DIR, \"Comparison\", \"quantitative_results.csv\")}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 7-3. í‰ê·  ë©”íŠ¸ë¦­ ìš”ì•½ í…Œì´ë¸”\n",
    "# ============================================================\n",
    "summary_metrics = {}\n",
    "\n",
    "for method in ['hrn', 'codeformer', 'gfpgan']:\n",
    "    psnr_col = f'{method}_psnr'\n",
    "    ssim_col = f'{method}_ssim'\n",
    "    lpips_col = f'{method}_lpips'\n",
    "\n",
    "    if psnr_col in df.columns:\n",
    "        summary_metrics[method.upper()] = {\n",
    "            'PSNR (dB) â†‘': df[psnr_col].dropna().mean(),\n",
    "            'SSIM â†‘': df[ssim_col].dropna().mean(),\n",
    "            'LPIPS â†“': df[lpips_col].dropna().mean(),\n",
    "        }\n",
    "\n",
    "summary_df = pd.DataFrame(summary_metrics).T\n",
    "print('=' * 60)\n",
    "print('          í‰ê·  ë©”íŠ¸ë¦­ ë¹„êµ ìš”ì•½')\n",
    "print('=' * 60)\n",
    "print(summary_df.to_string())\n",
    "print('=' * 60)\n",
    "print('â†‘ = ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, â†“ = ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ')\n",
    "\n",
    "summary_df.to_csv(os.path.join(RESULTS_DIR, 'Comparison', 'summary_metrics.csv'))\n",
    "print(f'\\nì €ì¥: {os.path.join(RESULTS_DIR, \"Comparison\", \"summary_metrics.csv\")}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 7-4. ì‹œê°ì  ë¹„êµ (ì •ì„±ì  í‰ê°€)\n",
    "# ============================================================\n",
    "# CT ë°ì´í„°ì— ëŒ€í•œ ë¹„êµ ì‹œê°í™”\n",
    "ct_image_ids = [k for k in all_test_images.keys() if k.startswith('ct_')]\n",
    "\n",
    "n_display = min(len(ct_image_ids), 6)\n",
    "if n_display == 0:\n",
    "    print('í‘œì‹œí•  CT ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.')\n",
    "else:\n",
    "    fig, axes = plt.subplots(n_display, 4, figsize=(20, 5 * n_display))\n",
    "    if n_display == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "\n",
    "    col_titles = ['Original (CT)', 'HRN (3D Recon)', 'CodeFormer', 'GFPGAN']\n",
    "\n",
    "    for i, img_id in enumerate(ct_image_ids[:n_display]):\n",
    "        original_path = all_test_images[img_id]\n",
    "\n",
    "        # Original\n",
    "        orig = cv2.imread(original_path)\n",
    "        if orig is not None:\n",
    "            axes[i, 0].imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB) if len(orig.shape) == 3 else orig, cmap='gray')\n",
    "        axes[i, 0].set_title(f'{col_titles[0]}\\n{img_id}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # HRN\n",
    "        patient_id = img_id.replace('ct_', '')\n",
    "        hrn_info = hrn_results.get(patient_id, {})\n",
    "        vis_path = hrn_info.get('saved_files', {}).get('visualization') if isinstance(hrn_info, dict) else None\n",
    "        if vis_path and os.path.exists(vis_path):\n",
    "            hrn_img = cv2.imread(vis_path)\n",
    "            axes[i, 1].imshow(cv2.cvtColor(hrn_img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            axes[i, 1].text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=20)\n",
    "        axes[i, 1].set_title(col_titles[1])\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # CodeFormer\n",
    "        cf_path = codeformer_results.get(img_id)\n",
    "        if cf_path and os.path.exists(cf_path):\n",
    "            cf_img = cv2.imread(cf_path)\n",
    "            axes[i, 2].imshow(cv2.cvtColor(cf_img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            axes[i, 2].text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=20)\n",
    "        axes[i, 2].set_title(col_titles[2])\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        # GFPGAN\n",
    "        gf_path = gfpgan_results.get(img_id)\n",
    "        if gf_path and os.path.exists(gf_path):\n",
    "            gf_img = cv2.imread(gf_path)\n",
    "            axes[i, 3].imshow(cv2.cvtColor(gf_img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            axes[i, 3].text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=20)\n",
    "        axes[i, 3].set_title(col_titles[3])\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "    plt.suptitle('ì–¼êµ´ ë³µì› ë°©ë²• ë¹„êµ (CT ë°ì´í„°)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'Comparison', 'visual_comparison_ct.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 7-5. ë©”íŠ¸ë¦­ ë¹„êµ ì°¨íŠ¸\n",
    "# ============================================================\n",
    "if summary_df.empty:\n",
    "    print('ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    methods = list(summary_df.index)\n",
    "    colors = ['#2196F3', '#FF9800', '#4CAF50']\n",
    "\n",
    "    # PSNR ë¹„êµ\n",
    "    psnr_vals = summary_df['PSNR (dB) â†‘'].values\n",
    "    axes[0].bar(methods, psnr_vals, color=colors)\n",
    "    axes[0].set_title('PSNR (dB) â†‘', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('dB')\n",
    "    for j, v in enumerate(psnr_vals):\n",
    "        if not np.isnan(v):\n",
    "            axes[0].text(j, v + 0.3, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "    # SSIM ë¹„êµ\n",
    "    ssim_vals = summary_df['SSIM â†‘'].values\n",
    "    axes[1].bar(methods, ssim_vals, color=colors)\n",
    "    axes[1].set_title('SSIM â†‘', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    for j, v in enumerate(ssim_vals):\n",
    "        if not np.isnan(v):\n",
    "            axes[1].text(j, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "    # LPIPS ë¹„êµ\n",
    "    lpips_vals = summary_df['LPIPS â†“'].values\n",
    "    axes[2].bar(methods, lpips_vals, color=colors)\n",
    "    axes[2].set_title('LPIPS â†“', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('Distance')\n",
    "    for j, v in enumerate(lpips_vals):\n",
    "        if not np.isnan(v):\n",
    "            axes[2].text(j, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Face Restoration Methods - Quantitative Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'Comparison', 'metrics_comparison_chart.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 8. ê²°ë¡  ë° ë¶„ì„\n",
    "\n",
    "### ì‹¤í—˜ ìš”ì•½\n",
    "\n",
    "| ë°©ë²• | ìœ í˜• | í•µì‹¬ ê¸°ìˆ  | ì¥ì  | ë‹¨ì  |\n",
    "|------|------|-----------|------|------|\n",
    "| **HRN** (ë…¼ë¬¸) | 2Dâ†’3D ë³µì› | ê³„ì¸µì  í‘œí˜„ (Low/Mid/High freq) + 3DMM | ê³ í’ˆì§ˆ 3D mesh, ë””í…Œì¼ ë¶„ë¦¬ | ì •ë©´ ì–¼êµ´ ì´ë¯¸ì§€ í•„ìš” |\n",
    "| **CodeFormer** | 2Dâ†’2D ë³µì› | Codebook + Transformer | ìì—°ìŠ¤ëŸ¬ìš´ ë³µì›, fidelity ì¡°ì ˆ ê°€ëŠ¥ | 3D ì •ë³´ ì—†ìŒ |\n",
    "| **GFPGAN** | 2Dâ†’2D ë³µì› | GAN + Facial Prior | ë¹ ë¥¸ ì¶”ë¡  ì†ë„ | ê³¼ë„í•œ smoothing ê°€ëŠ¥ |\n",
    "\n",
    "### í•µì‹¬ ë°œê²¬\n",
    "1. **HRN**ì€ ë‹¨ì¼ 2D ì´ë¯¸ì§€ì—ì„œ mid/high-frequency ë””í…Œì¼ì„ í¬í•¨í•œ ì •í™•í•œ 3D ì–¼êµ´ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ\n",
    "2. **CodeFormer/GFPGAN**ì€ 2D ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒì— íš¨ê³¼ì ì´ë‚˜, 3D ê¸°í•˜í•™ì  ì •ë³´ëŠ” ì œê³µí•˜ì§€ ì•ŠìŒ\n",
    "3. CT ë°ì´í„°ì—ì„œ ì¶”ì¶œí•œ 2D íˆ¬ì˜ ì´ë¯¸ì§€ëŠ” ì¼ë°˜ ì‚¬ì§„ê³¼ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì ¸ ëª¨ë“  ë°©ë²•ì—ì„œ ì¶”ê°€ ì „ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ\n",
    "4. FaceScape ë°ì´í„°ì—ì„œëŠ” HRNì´ ë…¼ë¬¸ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ\n",
    "\n",
    "### ê²°ê³¼ ì €ì¥ ìœ„ì¹˜\n",
    "ëª¨ë“  ê²°ê³¼ëŠ” Google Driveì˜ `BISTOOL_Results/` í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n",
    "- `HRN/`: 3D mesh (.obj), í…ìŠ¤ì²˜, ì‹œê°í™”\n",
    "- `CodeFormer/`: ë³µì›ëœ 2D ì´ë¯¸ì§€\n",
    "- `GFPGAN/`: ë³µì›ëœ 2D ì´ë¯¸ì§€\n",
    "- `Comparison/`: ì •ëŸ‰ì  ë¹„êµ CSV, ì‹œê°ì  ë¹„êµ ì´ë¯¸ì§€"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 8. CT 3D í‘œë©´ vs HRN 3D ë³µì› ë¹„êµ (ì¶”ê°€ ë¶„ì„)\n",
    "# ============================================================\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def chamfer_distance(verts1, verts2, num_samples=10000):\n",
    "    \"\"\"ë‘ ë©”ì‰¬ ê°„ì˜ Chamfer Distanceë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    idx1 = np.random.choice(len(verts1), min(num_samples, len(verts1)), replace=False)\n",
    "    idx2 = np.random.choice(len(verts2), min(num_samples, len(verts2)), replace=False)\n",
    "    v1, v2 = verts1[idx1], verts2[idx2]\n",
    "    tree1, tree2 = cKDTree(v1), cKDTree(v2)\n",
    "    dist1, _ = tree2.query(v1)\n",
    "    dist2, _ = tree1.query(v2)\n",
    "    return np.mean(dist1) + np.mean(dist2)\n",
    "\n",
    "print('CT 3D í‘œë©´ vs HRN 3D ë³µì› ë©”ì‰¬ ê¸°í•˜í•™ì  ë¹„êµ')\n",
    "print('=' * 60)\n",
    "\n",
    "geo_comparison = []\n",
    "for patient_id in ct_folders.keys():\n",
    "    hrn_info = hrn_results.get(patient_id, {})\n",
    "    mesh_path = hrn_info.get('saved_files', {}).get('mesh') if isinstance(hrn_info, dict) else None\n",
    "\n",
    "    if mesh_path and os.path.exists(mesh_path):\n",
    "        try:\n",
    "            hrn_mesh = trimesh.load(mesh_path)\n",
    "            # â˜… ct_foldersëŠ” ì´ë¯¸ ë¡œì»¬ ê²½ë¡œë¥¼ ê°€ë¦¬í‚´ (ë¹ ë¦„)\n",
    "            vol, sp, _ = load_dicom_series(ct_folders[patient_id]['path'])\n",
    "            ct_verts, ct_faces, _ = extract_face_surface(vol, sp, threshold=200)\n",
    "            del vol\n",
    "            cd = chamfer_distance(ct_verts, hrn_mesh.vertices)\n",
    "            geo_comparison.append({\n",
    "                'patient_id': patient_id,\n",
    "                'ct_vertices': ct_verts.shape[0],\n",
    "                'hrn_vertices': hrn_mesh.vertices.shape[0],\n",
    "                'chamfer_distance': cd,\n",
    "            })\n",
    "            print(f'  Patient {patient_id}: CD = {cd:.4f} mm')\n",
    "        except Exception as e:\n",
    "            print(f'  Patient {patient_id}: ì˜¤ë¥˜ - {e}')\n",
    "\n",
    "if geo_comparison:\n",
    "    geo_df = pd.DataFrame(geo_comparison)\n",
    "    print(f'\\ní‰ê·  Chamfer Distance: {geo_df[\"chamfer_distance\"].mean():.4f} mm')\n",
    "    geo_df.to_csv(os.path.join(RESULTS_DIR, 'Comparison', 'geometric_comparison.csv'), index=False)\n",
    "else:\n",
    "    print('ê¸°í•˜í•™ì  ë¹„êµ ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "# ============================================================\n",
    "print('=' * 70)\n",
    "print('          HRN Face Reconstruction - ì‹¤í—˜ ì™„ë£Œ')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print(f'ì²˜ë¦¬ëœ CT í™˜ì ìˆ˜: {len(ct_folders)}')\n",
    "print(f'HRN ì„±ê³µ ê±´ìˆ˜: {len([v for v in hrn_results.values() if isinstance(v, dict) and \"error\" not in v])}')\n",
    "print(f'CodeFormer ì„±ê³µ ê±´ìˆ˜: {len(codeformer_results)}')\n",
    "print(f'GFPGAN ì„±ê³µ ê±´ìˆ˜: {len(gfpgan_results)}')\n",
    "print()\n",
    "print(f'ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {RESULTS_DIR}')\n",
    "print()\n",
    "print('ì €ì¥ëœ ê²°ê³¼ë¬¼:')\n",
    "for root, dirs, files in os.walk(RESULTS_DIR):\n",
    "    for f in files:\n",
    "        rel_path = os.path.relpath(os.path.join(root, f), RESULTS_DIR)\n",
    "        size_kb = os.path.getsize(os.path.join(root, f)) / 1024\n",
    "        print(f'  {rel_path}: {size_kb:.1f} KB')\n",
    "print()\n",
    "print('ì‹¤í—˜ ì™„ë£Œ!')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}